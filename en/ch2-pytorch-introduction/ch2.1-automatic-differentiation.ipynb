{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9139deae",
   "metadata": {},
   "source": [
    "# 2.1 Automatic Differentiation in PyTorch\n",
    "\n",
    "In Section 1.3, we described the computation graph as a “chain of responsibility”: if the loss takes on a certain value, we can trace backward along the chain to determine how much each parameter is “responsible.” In this section, we switch to a more engineering-oriented perspective: How does a deep learning framework automatically build this chain of responsibility, and compute gradients when needed?\n",
    "\n",
    "Let’s state the problem more plainly: during training, what we need are gradients. But what we actually write is just code — additions, multiplications, convolutions, activation functions… These operations execute line by line during the forward pass and eventually produce a `loss`. So where do the gradients come from? Does the framework symbolically derive one gigantic expression?\n",
    "\n",
    "Of course not. What deep learning frameworks actually do is more like this:\n",
    "\n",
    "- During the forward pass, they keep track of what operations you performed, who depends on whom, and what the intermediate results are.\n",
    "- During the backward pass, they trace back along this “ledger”: starting from the `loss`, they go backward, and whenever they encounter an operation, they use its own “local derivative rule” to pass the gradient further down.\n",
    "\n",
    "Understanding this mechanism is crucial. It not only explains “where the gradients come from,” but also directly impacts many phenomena we will encounter later: such as why gradients accumulate, why intermediate variables don’t have a `.grad` attribute by default, why some operations can cut off the gradient chain, and why there’s always a trade-off between memory and computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341d5119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd.functional as AF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdc078",
   "metadata": {},
   "source": [
    "## 2.1.1 A Computation Graph is Not Drawn — It is Executed\n",
    "\n",
    "To understand automatic differentiation in PyTorch, the best approach is not to memorize definitions first, but to observe one simple fact: you are only performing forward computation — yet the computation graph is automatically constructed during execution.\n",
    "\n",
    "Suppose we have a simple function:\n",
    "\n",
    "$$ z = \\sin(x \\cdot y) $$\n",
    "\n",
    "We can decompose it into two basic steps:\n",
    "\n",
    "1. Compute the dot product: $q = x \\cdot y$\n",
    "2. Apply the sine function: $z = \\sin(q)$\n",
    "\n",
    "Now we tell PyTorch that we want to compute the gradients of `z` with respect to `x` and `y`.\n",
    "\n",
    "When we create tensors with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8c612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(1.0, 5.0, requires_grad=True)\n",
    "y = torch.arange(5.0, 9.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d072f1c",
   "metadata": {},
   "source": [
    "The argument `requires_grad=True` can be understood as a declaration: these variables need to be “held accountable.” From that point on, any result computed from them will automatically inherit the ability to require gradients, and PyTorch will internally record:\n",
    "\n",
    "- Which operation produced this tensor?\n",
    "- Which tensors did it depend on?\n",
    "\n",
    "Now we perform two ordinary forward computations:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e937b0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "q = torch.dot(x, y)\n",
    "z = torch.sin(q)\n",
    "print('z.requires_grad:', z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d3da5",
   "metadata": {},
   "source": [
    "At the surface level, this still looks like pure numerical computation. But internally, PyTorch has already done two important things:\n",
    "\n",
    "- `z` automatically becomes a tensor that requires gradients (because it depends on `x` and `y`, which require gradients).\n",
    "- he creation process of `q` and `z` is recorded: `z` is produced by a sin operation, `q` is produced by a dot operation, and `q` depends on `x` and `y`.\n",
    "\n",
    "At this stage, don’t worry about what the computation graph “looks like.” Instead, observe a more intuitive phenomenon: before you explicitly trigger backpropagation, gradients do not magically appear.\n",
    "\n",
    "If we inspect:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eda64eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: None\n",
      "y.grad: None\n"
     ]
    }
   ],
   "source": [
    "print('x.grad:', x.grad)\n",
    "print('y.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3391b1a4",
   "metadata": {},
   "source": [
    "we will see it is not zero — but None. This makes sense.\n",
    "\n",
    "Gradients are the result of a backward traversal. Only when you explicitly initiate that traversal (for example, by calling `backward()`), will PyTorch follow the recorded dependency structure, compute gradients, and write them back to the leaf nodes.\n",
    "\n",
    "If you do not call `backward()`, PyTorch will not compute gradients — and therefore nothing will be populated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c07c86",
   "metadata": {},
   "source": [
    "## 2.1.2 What Does `backward()` Actually Do?\n",
    "\n",
    "In the previous section, we only performed forward computation, but PyTorch had already silently recorded the dependency structure. Now the real question is: when we call `backward()`, what exactly does the framework do? And can we trust the gradients it computes?\n",
    "\n",
    "We continue with the same example:\n",
    "\n",
    "$$ q = x^\\top y, \\quad z = \\sin(q) $$\n",
    "\n",
    "If we compute the gradients manually, we obtain:\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial q} \\cdot \\frac{\\partial q}{\\partial x} = \\cos(q) \\cdot y $$\n",
    "$$ \\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial q} \\cdot \\frac{\\partial q}{\\partial y} = \\cos(q) \\cdot x $$\n",
    "\n",
    "Now let PyTorch do the computation. We initiate backpropagation from the output `z`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62cbabcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([3.1666, 3.7999, 4.4332, 5.0666])\n",
      "y.grad: tensor([0.6333, 1.2666, 1.9000, 2.5333])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print('x.grad:', x.grad)\n",
    "print('y.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892bac57",
   "metadata": {},
   "source": [
    "After this call, `.grad` is no longer `None`. The gradients have been written back to the leaf tensors `x` and `y`. Intuitively, you can think of `backward()` like this:\n",
    "\n",
    "1. Start from `z`, and assume by default that $\\frac{\\partial z}{\\partial z} = 1$;\n",
    "2. Follow the dependency chain recorded during the forward pass, but in reverse;\n",
    "3. At each operator node, apply its **local derivative rule**, and propagate the gradient upstream.\n",
    "\n",
    "We can verify that PyTorch’s result matches our manual derivation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "757e46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(x.grad, y * x.dot(y).cos())\n",
    "assert torch.allclose(y.grad, x * x.dot(y).cos())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71eb159",
   "metadata": {},
   "source": [
    "At this point, the core idea of automatic differentiation should be clear: the framework does not derive one giant symbolic expression. It only needs to know how to differentiate each primitive operation locally, and then chain these local rules together according to the computation graph.\n",
    "\n",
    "**Looking Inside the Graph: `grad_fn`**\n",
    "\n",
    "If we dig a little deeper, PyTorch exposes part of this backward chain:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccddf67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.grad_fn: SinBackward0\n",
      "q.grad_fn: DotBackward0\n",
      "x.grad_fn: None\n",
      "y.grad_fn: None\n"
     ]
    }
   ],
   "source": [
    "print('z.grad_fn:', z.grad_fn.name())\n",
    "print('q.grad_fn:', q.grad_fn.name())\n",
    "print('x.grad_fn:', x.grad_fn)\n",
    "print('y.grad_fn:', y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a208347",
   "metadata": {},
   "source": [
    "You will see names such as `SinBackward0` and `DotBackward0`. These names can be roughly interpreted as:\n",
    "\n",
    "- `z` is not created out of thin air; it is produced by an operator (here, `sin`).\n",
    "- `grad_fn` is the corresponding backward function object for that operator.\n",
    "\n",
    "During backpropagation, PyTorch starts from the root node and calls each node’s backward function in sequence:\n",
    "\n",
    "- When you call `z.backward()`, PyTorch first invokes `SinBackward0`, which computes $\\frac{\\partial z}{\\partial q}$.\n",
    "- It then passes this value to `DotBackward0`, which computes $\\frac{\\partial q}{\\partial x}$ and $\\frac{\\partial q}{\\partial y}$.\n",
    "- Finally, the gradients $\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial y}$ are accumulated into the leaf tensors.\n",
    "\n",
    "Leaf tensors (`x`, `y`) do not have `grad_fn` because they are the starting points of the graph — there is nothing upstream to differentiate.\n",
    "\n",
    "**`next_functions`: Who is Upstream?**\n",
    "\n",
    "Each backward node also keeps track of its upstream dependencies:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a72e740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fn of z.child -> q: DotBackward0\n",
      "grad_fn of q.child -> x: struct torch::autograd::AccumulateGrad\n",
      "grad_fn of q.child -> y: struct torch::autograd::AccumulateGrad\n"
     ]
    }
   ],
   "source": [
    "node_q = z.grad_fn.next_functions[0][0]\n",
    "node_x = node_q.next_functions[0][0]\n",
    "node_y = node_q.next_functions[1][0]\n",
    "print('grad_fn of z.child -> q:', node_q.name())\n",
    "print('grad_fn of q.child -> x:', node_x.name())\n",
    "print('grad_fn of q.child -> y:', node_y.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49447bd3",
   "metadata": {},
   "source": [
    "These connections describe where the backward pass should go next. For example, `SinBackward0` points to `DotBackward0`, `DotBackward0` points to two special nodes called `AccumulateGrad`.\n",
    "\n",
    "`AccumulateGrad` is a special node type. Each leaf tensor that requires gradients has an associated `AccumulateGrad` node. Its job is simple: take the computed gradient and accumulate it into the tensor’s `.grad` attribute. That is why `x.grad` and `y.grad` become populated only after calling `backward()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e529c",
   "metadata": {},
   "source": [
    "## 2.1.3 Why Non-Scalar Outputs Cannot Call `backward()` Directly\n",
    "\n",
    "In the previous example, `z` was a scalar. That is why we could confidently write `z.backward()`. However, many people will immediately encounter a seemingly unreasonable limitation the first time they replace a scalar output with a vector or matrix:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8937c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: grad can be implicitly created only for scalar outputs\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1.0, 5.0, requires_grad=True)\n",
    "y = torch.arange(5.0, 9.0, requires_grad=True)\n",
    "Z = torch.outer(x, y)\n",
    "try:\n",
    "    Z.backward()  # This will raise an error because z is not a scalar\n",
    "except RuntimeError as err:\n",
    "    print('RuntimeError:', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243b5b6",
   "metadata": {},
   "source": [
    "This is not PyTorch being unreasonable. The reason is that the starting point of backpropagation is no longer unique when the output is not a scalar.\n",
    "\n",
    "For a scalar `z`, we usually care about $\\frac{\\partial z}{\\partial x}$ and $\\frac{\\partial z}{\\partial y}$. The backward pass starts from the output, and the first step is to set $\\frac{\\partial z}{\\partial z} = 1$. This step is reasonable because the unit gradient for a scalar output is unambiguous: we want to backpropagate along the direction of `z`.\n",
    "\n",
    "But what if the output is a vector or matrix `Z`? What exactly do we want?\n",
    "\n",
    "- Do we want the gradient of each element of `Z` with respect to `x` and `y`? That would be a higher-order tensor.\n",
    "- Or do we want some scalar function, such as the sum, mean, or a weighted sum of `Z`, with respect to `x` and `y`?\n",
    "\n",
    "In other words, for non-scalar outputs, backpropagation must first answer one question: from which “direction” do we propagate the gradient?\n",
    "\n",
    "Mathematically, this “direction” is a tensor `v` with the same shape as the output:\n",
    "\n",
    "$$ v = \\frac{\\partial L}{\\partial Z} $$\n",
    "\n",
    "Then PyTorch actually computes a vector–Jacobian product (VJP):\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial x} = v^\\top \\left(\\frac{\\partial Z}{\\partial x}\\right) $$\n",
    "\n",
    "For scalar outputs, `v` is automatically 1 (equivalent to calling `Z.backward()`, which takes $L$ as $Z$); for non-scalar outputs, `v` needs to be provided by ourselves.\n",
    "\n",
    "There are two ways to write this.\n",
    "\n",
    "One way is to explicitly pass `gradient`, indicating the direction along which we want to propagate:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef9af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([26., 26., 26., 26.])\n",
      "y.grad: tensor([10., 10., 10., 10.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1.0, 5.0, requires_grad=True)\n",
    "y = torch.arange(5.0, 9.0, requires_grad=True)\n",
    "Z = torch.outer(x, y)\n",
    "Z.backward(gradient=torch.ones_like(Z))\n",
    "print('x.grad:', x.grad)\n",
    "print('y.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b0c2d",
   "metadata": {},
   "source": [
    "Here, `torch.ones_like(Z)` means that\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial Z_{i,j}} = 1. $$\n",
    "\n",
    "So passing an all-ones gradient is equivalent to defining\n",
    "\n",
    "$$ L = \\sum_{i,j} Z_{i,j} $$\n",
    "\n",
    "and then calling `backward()`.\n",
    "\n",
    "Another way is to first convert `Z` into a scalar and then call `backward()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974dd203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([26., 26., 26., 26.])\n",
      "y.grad: tensor([10., 10., 10., 10.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1.0, 5.0, requires_grad=True)\n",
    "y = torch.arange(5.0, 9.0, requires_grad=True)\n",
    "Z = torch.outer(x, y)\n",
    "Z = torch.sum(Z)\n",
    "Z.backward()\n",
    "print('x.grad:', x.grad)\n",
    "print('y.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00fe68",
   "metadata": {},
   "source": [
    "In many cases, these two approaches are equivalent. Either we explicitly tell PyTorch along which direction to propagate gradients, or we first reduce the output to a scalar (for example, by summing), so that it implicitly propagates along that scalar direction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a50d21",
   "metadata": {},
   "source": [
    "## 2.1.4 Higher-Order Derivatives: Making the Differentiation Process Itself Part of the Computation\n",
    "\n",
    "Up to this point, what we have computed are first-order gradients: given a scalar output (or something that can be converted into a scalar) $L$, we compute $\\nabla_x L$ and $\\nabla_y L$. But sometimes we need higher-order information, such as second derivatives (certain directions of the Hessian), curvature, or for use in some regularization terms.\n",
    "\n",
    "The key point here is: if you want to differentiate a “gradient”, then the process of computing that gradient must itself be differentiable. This is the meaning of `create_graph=True`. When computing first-order derivatives, we not only compute numerical values, but also record the process of computing those derivatives as a new computation graph.\n",
    "\n",
    "At this point, many people may ask: why not use `backward()`? Because the design goal of `backward()` is training the model. It accumulates gradients into the `.grad` attributes of leaf tensors and, by default, frees the computation graph to save memory. But when computing higher-order derivatives, we usually want:\n",
    "\n",
    "- The gradient to be returned as a tensor (so that it can be further computed).\n",
    "- To optionally retain / construct the computation graph (so that we can differentiate again).\n",
    "\n",
    "Therefore, `torch.autograd.grad` is more commonly used.\n",
    "\n",
    "We continue to use the same example: $z = \\sin(x \\cdot y)$. We first compute the first-order derivatives $\\frac{dz}{dx}$ and $\\frac{dz}{dy}$, and then differentiate these results to see what the second-order derivatives $\\frac{d^2 z}{dx^2}$ and $\\frac{d^2 z}{dy^2}$ look like.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c793f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(-0.5820, grad_fn=<MulBackward0>)\n",
      "dz/dy: tensor(-0.2910, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(4.0, requires_grad=True)\n",
    "z = torch.sin(x * y)\n",
    "\n",
    "dzdx, dzdy = torch.autograd.grad(z, (x, y), create_graph=True)\n",
    "print('dz/dx:', dzdx)\n",
    "print('dz/dy:', dzdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda58929",
   "metadata": {},
   "source": [
    "The most important line is `create_graph=True`. Without it, `dz/dx` and `dz/dy` would be treated as pure numerical results, and the information about how they were computed would not be retained. Then we would not be able to differentiate them again. The outputs `dz/dx` and `dz/dy `both contain a `grad_fn`, which indicates that they themselves can be differentiated.\n",
    "\n",
    "When computing higher-order derivatives, we sometimes want to compute gradients with respect to different variables sequentially within the same computation graph. However, after calling `backward()`, PyTorch by default frees the computation graph to save memory, which prevents further backward passes on the same graph. If we truly need to perform multiple backward passes on the same forward result, we can set `retain_graph=True` to retain the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5512cdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(-0.5820, grad_fn=<MulBackward0>)\n",
      "dz/dy: tensor(-0.2910, grad_fn=<MulBackward0>)\n",
      "d2z/dx2: tensor(-15.8297)\n",
      "d2z/dy2: tensor(-3.9574)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(4.0, requires_grad=True)\n",
    "z = torch.sin(x * y)\n",
    "\n",
    "dzdx, dzdy = torch.autograd.grad(z, (x, y), create_graph=True)\n",
    "print('dz/dx:', dzdx)\n",
    "print('dz/dy:', dzdy)\n",
    "\n",
    "(d2zdx2,) = torch.autograd.grad(dzdx, x, retain_graph=True)\n",
    "(d2zdy2,) = torch.autograd.grad(dzdy, y)\n",
    "print('d2z/dx2:', d2zdx2)\n",
    "print('d2z/dy2:', d2zdy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4272db",
   "metadata": {},
   "source": [
    "However, a more common practice is to execute the forward pass again to obtain a new computation graph. `retain_graph=True` is typically used only when we genuinely need to perform multiple gradient computations on the same graph, such as in experiments with higher-order derivatives or in certain regularization terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee745fc6",
   "metadata": {},
   "source": [
    "## 2.1.5 VJP and JVP: What Reverse Mode and Forward Mode Actually Compute\n",
    "\n",
    "Up to this point, we have been talking about “computing gradients”. But strictly speaking, most functions in deep learning are not scalar-to-scalar mappings. Instead, they are:\n",
    "\n",
    "$$ f: \\mathbb{R}^n \\to \\mathbb{R}^m $$\n",
    "\n",
    "Its derivative is a Jacobian matrix:\n",
    "\n",
    "$$ J = \\frac{\\partial f}{\\partial x} \\in \\mathbb{R}^{m \\times n} $$\n",
    "\n",
    "The real issue is that when both $m,n$ are large, we almost never explicitly construct $J$. What we actually want, and what the framework computes in practice, is a product involving the Jacobian — either multiplied on the left or on the right.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4c3a3",
   "metadata": {},
   "source": [
    "### 2.1.5.1 VJP: Vector–Jacobian Product (Reverse Mode)\n",
    "\n",
    "Given an “upstream gradient” vector $v \\in \\mathbb{R}^m$ (which can be understood as $\\frac{\\partial L}{\\partial f}$), reverse mode computes:\n",
    "\n",
    "$$ v^\\top J \\in \\mathbb{R}^n $$\n",
    "\n",
    "This is called the **vector–Jacobian product (VJP)**.\n",
    "\n",
    "In the language of training:\n",
    "\n",
    "- We have a scalar `loss`: $L = \\mathcal{L}(f(x))$\n",
    "- An upstream gradient: $v = \\frac{\\partial L}{\\partial f}$\n",
    "- Backpropagation computes: $\\frac{\\partial L}{\\partial x} = v^\\top \\frac{\\partial f}{\\partial x}$\n",
    "\n",
    "So when we call `backward()` in practice, what is actually being computed is a special case of VJP.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa45c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func(x,y): tensor(0.7739)\n",
      "VJP output: (tensor([3.1666, 3.7999, 4.4332, 5.0666]), tensor([0.6333, 1.2666, 1.9000, 2.5333]))\n"
     ]
    }
   ],
   "source": [
    "def vjp_func(x: torch.Tensor, y: torch.Tensor):\n",
    "    return torch.sin(torch.dot(x, y))\n",
    "\n",
    "\n",
    "x = torch.arange(1.0, 5.0)\n",
    "y = torch.arange(5.0, 9.0)\n",
    "out = AF.vjp(vjp_func, (x, y))\n",
    "print('func(x,y):', out[0])\n",
    "print('VJP output:', out[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfb5b8",
   "metadata": {},
   "source": [
    "### 2.1.5.2 JVP: Jacobian–Vector Product (Forward Mode)\n",
    "\n",
    "Forward mode is the opposite. Given an input direction $u \\in \\mathbb{R}^n$, it computes:\n",
    "\n",
    "$$ Ju \\in \\mathbb{R}^m $$\n",
    "\n",
    "This is called the **Jacobian–vector product (JVP)**.\n",
    "\n",
    "Intuitively, it answers the following question: if we make a small perturbation in the input space along direction $u$, along which direction will the output change? This is common in sensitivity analysis, implicit layers, certain second-order methods, and some physics or scientific computing settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1a51552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func(x,y): tensor(0.7739)\n",
      "JVP output: tensor(2.9133)\n"
     ]
    }
   ],
   "source": [
    "def jvp_func(a: torch.Tensor, b: torch.Tensor):\n",
    "    return torch.sin(torch.dot(a, b))\n",
    "\n",
    "\n",
    "x = torch.arange(1.0, 5.0)\n",
    "y = torch.arange(5.0, 9.0)\n",
    "v_x = torch.full_like(x, 0.1)\n",
    "v_y = torch.full_like(y, 0.2)\n",
    "out = AF.jvp(jvp_func, (x, y), (v_x, v_y))\n",
    "print('func(x,y):', out[0])\n",
    "print('JVP output:', out[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab0c2d",
   "metadata": {},
   "source": [
    "### 2.1.5.3 Why VJP Is More Common in Deep Learning\n",
    "\n",
    "This is not about which method is “more advanced,” but about scale matching.\n",
    "\n",
    "- In deep learning training, $n$ is usually the parameter dimension (millions or billions).\n",
    "- $m$ is usually the output dimension (often a scalar).\n",
    "- What we want is $\\nabla_x L \\in \\mathbb{R}^n$.\n",
    "\n",
    "The computational cost of VJP is roughly on the order of one backward pass. It is suitable when $n$ is large but the output is scalar or low-dimensional. JVP is more suitable when the input dimension is relatively small, but we care about how the output changes along certain directions.\n",
    "\n",
    "Therefore, a common rule of thumb is:\n",
    "\n",
    "- If the output is scalar or low-dimensional and the input dimension is large, reverse mode (VJP) is more appropriate.\n",
    "- If the input dimension is relatively small and the output dimension is large, forward mode (JVP) may be more appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8126a6",
   "metadata": {},
   "source": [
    "## 2.1.6 Common Errors in Backpropagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578f62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(1.0, 5.0, requires_grad=True)\n",
    "y = torch.arange(5.0, 9.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d5207",
   "metadata": {},
   "source": [
    "**Calling `backward()` multiple times**\n",
    "\n",
    "Calling `backward()` multiple times on the same computation graph will cause an error. After the first backward pass finishes, PyTorch frees the intermediate tensors that were saved only for backpropagation, in order to save memory. Therefore, when we try to traverse the same graph a second time, we find that the “markers” have already been cleared. If multiple gradient computations are needed, you can set `retain_graph=True` in the first call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfa70386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"
     ]
    }
   ],
   "source": [
    "z = torch.sin(torch.dot(x, y))\n",
    "z.backward()\n",
    "try:\n",
    "    z.backward()  # This will raise an error because gradients are already computed\n",
    "except RuntimeError as err:\n",
    "    print('RuntimeError:', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0d93e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.sin(torch.dot(x, y))\n",
    "z.backward(retain_graph=True)\n",
    "z.backward()  # This works because we retained the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceda4bd",
   "metadata": {},
   "source": [
    "**Trying to access the gradient of intermediate nodes**\n",
    "\n",
    "Only leaf nodes (that is, the original tensors created by the user) store gradient information. Gradients of intermediate nodes are not stored. If every intermediate tensor stored its gradient, memory usage would increase dramatically. Moreover, during training we only need parameter gradients, not gradients of all intermediate values. Therefore, attempting to access the `.grad` attribute of intermediate tensors will return `None` and may trigger a `UserWarning`. If you need to retain the gradient of an intermediate node, you can call `retain_grad()` on that tensor when it is created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5819a95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.grad: None\n",
      "UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:497.)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "q = torch.dot(x, y)\n",
    "z = torch.sin(q)\n",
    "z.backward()\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    print('q.grad:', q.grad)\n",
    "    if len(w) > 0:\n",
    "        for warn in w:\n",
    "            print('UserWarning:', warn.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8849d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.grad after retain_grad: tensor(0.6333)\n"
     ]
    }
   ],
   "source": [
    "q = torch.dot(x, y)\n",
    "q.retain_grad()\n",
    "z = torch.sin(q)\n",
    "z.backward()\n",
    "print('q.grad after retain_grad:', q.grad)  # Now q.grad is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc903e3",
   "metadata": {},
   "source": [
    "**Using in-place operations**\n",
    "\n",
    "In PyTorch, operations with a trailing underscore, such as `x.add_(1)` or `x.relu_()`, modify the tensor in place. They do not create a new tensor, but directly modify the memory of `x`. This may appear convenient, but during backpropagation, PyTorch often needs certain intermediate values from the forward pass. If those values are modified in place after the forward pass, the backward computation may lose the information required to compute gradients correctly. Therefore, during backpropagation, it is recommended to avoid in-place operations, or ensure that they do not modify intermediate variables required for gradient computation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42c36582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n"
     ]
    }
   ],
   "source": [
    "z = torch.dot(x, y)\n",
    "try:\n",
    "    x.relu_()\n",
    "except RuntimeError as err:\n",
    "    print('RuntimeError:', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9571ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.dot(x, y)\n",
    "x = torch.relu(x)\n",
    "z.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
