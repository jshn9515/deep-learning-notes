<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>2.1 Automatic Differentiation in PyTorch – deep-learning</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-8b4baf804e461d9b72633f0de59a0cac.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap-11430f55dfe4085c4a492823cb6ff128.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-sidebar floating nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">deep-learning</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../zh/"> 
<span class="menu-text">中文</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../en/"> 
<span class="menu-text">English</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../en/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html">Chapter 2: Introduction to PyTorch</a></li><li class="breadcrumb-item"><a href="../../en/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html">2.1 Automatic Differentiation in PyTorch</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Chapter 2: Introduction to PyTorch</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../../en/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">2.1 Automatic Differentiation in PyTorch</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a-computation-graph-is-not-drawn-it-is-executed" id="toc-a-computation-graph-is-not-drawn-it-is-executed" class="nav-link active" data-scroll-target="#a-computation-graph-is-not-drawn-it-is-executed">2.1.1 A Computation Graph is Not Drawn — It is Executed</a></li>
  <li><a href="#what-does-backward-actually-do" id="toc-what-does-backward-actually-do" class="nav-link" data-scroll-target="#what-does-backward-actually-do">2.1.2 What Does <code>backward()</code> Actually Do?</a></li>
  <li><a href="#why-non-scalar-outputs-cannot-call-backward-directly" id="toc-why-non-scalar-outputs-cannot-call-backward-directly" class="nav-link" data-scroll-target="#why-non-scalar-outputs-cannot-call-backward-directly">2.1.3 Why Non-Scalar Outputs Cannot Call <code>backward()</code> Directly</a></li>
  <li><a href="#higher-order-derivatives-making-the-differentiation-process-itself-part-of-the-computation" id="toc-higher-order-derivatives-making-the-differentiation-process-itself-part-of-the-computation" class="nav-link" data-scroll-target="#higher-order-derivatives-making-the-differentiation-process-itself-part-of-the-computation">2.1.4 Higher-Order Derivatives: Making the Differentiation Process Itself Part of the Computation</a></li>
  <li><a href="#vjp-and-jvp-what-reverse-mode-and-forward-mode-actually-compute" id="toc-vjp-and-jvp-what-reverse-mode-and-forward-mode-actually-compute" class="nav-link" data-scroll-target="#vjp-and-jvp-what-reverse-mode-and-forward-mode-actually-compute">2.1.5 VJP and JVP: What Reverse Mode and Forward Mode Actually Compute</a>
  <ul class="collapse">
  <li><a href="#vjp-vectorjacobian-product-reverse-mode" id="toc-vjp-vectorjacobian-product-reverse-mode" class="nav-link" data-scroll-target="#vjp-vectorjacobian-product-reverse-mode">2.1.5.1 VJP: Vector–Jacobian Product (Reverse Mode)</a></li>
  <li><a href="#jvp-jacobianvector-product-forward-mode" id="toc-jvp-jacobianvector-product-forward-mode" class="nav-link" data-scroll-target="#jvp-jacobianvector-product-forward-mode">2.1.5.2 JVP: Jacobian–Vector Product (Forward Mode)</a></li>
  <li><a href="#why-vjp-is-more-common-in-deep-learning" id="toc-why-vjp-is-more-common-in-deep-learning" class="nav-link" data-scroll-target="#why-vjp-is-more-common-in-deep-learning">2.1.5.3 Why VJP Is More Common in Deep Learning</a></li>
  </ul></li>
  <li><a href="#common-errors-in-backpropagation" id="toc-common-errors-in-backpropagation" class="nav-link" data-scroll-target="#common-errors-in-backpropagation">2.1.6 Common Errors in Backpropagation</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/jshn9515/deep-learning-notes/blob/main/en/ch2-pytorch-introduction/ch2.1-automatic-differentiation.ipynb" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/jshn9515/deep-learning-notes/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.dev/jshn9515/deep-learning-notes/blob/main/en/ch2-pytorch-introduction/ch2.1-automatic-differentiation.ipynb" class="toc-action"><i class="bi empty"></i>Edit this page</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../../en/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html">Chapter 2: Introduction to PyTorch</a></li><li class="breadcrumb-item"><a href="../../en/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html">2.1 Automatic Differentiation in PyTorch</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">2.1 Automatic Differentiation in PyTorch</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>In Section 1.3, we described the computation graph as a “chain of responsibility”: if the loss takes on a certain value, we can trace backward along the chain to determine how much each parameter is “responsible.” In this section, we switch to a more engineering-oriented perspective: How does a deep learning framework automatically build this chain of responsibility, and compute gradients when needed?</p>
<p>Let’s state the problem more plainly: during training, what we need are gradients. But what we actually write is just code — additions, multiplications, convolutions, activation functions… These operations execute line by line during the forward pass and eventually produce a <code>loss</code>. So where do the gradients come from? Does the framework symbolically derive one gigantic expression?</p>
<p>Of course not. What deep learning frameworks actually do is more like this:</p>
<ul>
<li>During the forward pass, they keep track of what operations you performed, who depends on whom, and what the intermediate results are.</li>
<li>During the backward pass, they trace back along this “ledger”: starting from the <code>loss</code>, they go backward, and whenever they encounter an operation, they use its own “local derivative rule” to pass the gradient further down.</li>
</ul>
<p>Understanding this mechanism is crucial. It not only explains “where the gradients come from,” but also directly impacts many phenomena we will encounter later: such as why gradients accumulate, why intermediate variables don’t have a <code>.grad</code> attribute by default, why some operations can cut off the gradient chain, and why there’s always a trade-off between memory and computation.</p>
<div id="341d5119" class="cell" data-execution_count="1">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch.autograd.functional <span class="im">as</span> AF</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<section id="a-computation-graph-is-not-drawn-it-is-executed" class="level2">
<h2 class="anchored" data-anchor-id="a-computation-graph-is-not-drawn-it-is-executed">2.1.1 A Computation Graph is Not Drawn — It is Executed</h2>
<p>To understand automatic differentiation in PyTorch, the best approach is not to memorize definitions first, but to observe one simple fact: you are only performing forward computation — yet the computation graph is automatically constructed during execution.</p>
<p>Suppose we have a simple function:</p>
<p><span class="math display">\[ z = \sin(x \cdot y) \]</span></p>
<p>We can decompose it into two basic steps:</p>
<ol type="1">
<li>Compute the dot product: <span class="math inline">\(q = x \cdot y\)</span></li>
<li>Apply the sine function: <span class="math inline">\(z = \sin(q)\)</span></li>
</ol>
<p>Now we tell PyTorch that we want to compute the gradients of <code>z</code> with respect to <code>x</code> and <code>y</code>.</p>
<p>When we create tensors with:</p>
<div id="0b8c612f" class="cell" data-execution_count="2">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="fl">1.0</span>, <span class="fl">5.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.arange(<span class="fl">5.0</span>, <span class="fl">9.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>The argument <code>requires_grad=True</code> can be understood as a declaration: these variables need to be “held accountable.” From that point on, any result computed from them will automatically inherit the ability to require gradients, and PyTorch will internally record:</p>
<ul>
<li>Which operation produced this tensor?</li>
<li>Which tensors did it depend on?</li>
</ul>
<p>Now we perform two ordinary forward computations:</p>
<div id="e937b0be" class="cell" data-execution_count="3">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> torch.dot(x, y)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.sin(q)</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'z.requires_grad:'</span>, z.requires_grad)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>z.requires_grad: True</code></pre>
</div>
</div>
<p>At the surface level, this still looks like pure numerical computation. But internally, PyTorch has already done two important things:</p>
<ul>
<li><code>z</code> automatically becomes a tensor that requires gradients (because it depends on <code>x</code> and <code>y</code>, which require gradients).</li>
<li>he creation process of <code>q</code> and <code>z</code> is recorded: <code>z</code> is produced by a sin operation, <code>q</code> is produced by a dot operation, and <code>q</code> depends on <code>x</code> and <code>y</code>.</li>
</ul>
<p>At this stage, don’t worry about what the computation graph “looks like.” Instead, observe a more intuitive phenomenon: before you explicitly trigger backpropagation, gradients do not magically appear.</p>
<p>If we inspect:</p>
<div id="2eda64eb" class="cell" data-execution_count="4">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x.grad:'</span>, x.grad)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'y.grad:'</span>, y.grad)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>x.grad: None
y.grad: None</code></pre>
</div>
</div>
<p>we will see it is not zero — but None. This makes sense.</p>
<p>Gradients are the result of a backward traversal. Only when you explicitly initiate that traversal (for example, by calling <code>backward()</code>), will PyTorch follow the recorded dependency structure, compute gradients, and write them back to the leaf nodes.</p>
<p>If you do not call <code>backward()</code>, PyTorch will not compute gradients — and therefore nothing will be populated.</p>
</section>
<section id="what-does-backward-actually-do" class="level2">
<h2 class="anchored" data-anchor-id="what-does-backward-actually-do">2.1.2 What Does <code>backward()</code> Actually Do?</h2>
<p>In the previous section, we only performed forward computation, but PyTorch had already silently recorded the dependency structure. Now the real question is: when we call <code>backward()</code>, what exactly does the framework do? And can we trust the gradients it computes?</p>
<p>We continue with the same example:</p>
<p><span class="math display">\[ q = x^\top y, \quad z = \sin(q) \]</span></p>
<p>If we compute the gradients manually, we obtain:</p>
<p><span class="math display">\[ \frac{\partial z}{\partial x} = \frac{\partial z}{\partial q} \cdot \frac{\partial q}{\partial x} = \cos(q) \cdot y \]</span> <span class="math display">\[ \frac{\partial z}{\partial y} = \frac{\partial z}{\partial q} \cdot \frac{\partial q}{\partial y} = \cos(q) \cdot x \]</span></p>
<p>Now let PyTorch do the computation. We initiate backpropagation from the output <code>z</code>:</p>
<div id="62cbabcd" class="cell" data-execution_count="5">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>z.backward()</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x.grad:'</span>, x.grad)</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'y.grad:'</span>, y.grad)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>x.grad: tensor([3.1666, 3.7999, 4.4332, 5.0666])
y.grad: tensor([0.6333, 1.2666, 1.9000, 2.5333])</code></pre>
</div>
</div>
<p>After this call, <code>.grad</code> is no longer <code>None</code>. The gradients have been written back to the leaf tensors <code>x</code> and <code>y</code>. Intuitively, you can think of <code>backward()</code> like this:</p>
<ol type="1">
<li>Start from <code>z</code>, and assume by default that <span class="math inline">\(\frac{\partial z}{\partial z} = 1\)</span>;</li>
<li>Follow the dependency chain recorded during the forward pass, but in reverse;</li>
<li>At each operator node, apply its <strong>local derivative rule</strong>, and propagate the gradient upstream.</li>
</ol>
<p>We can verify that PyTorch’s result matches our manual derivation:</p>
<div id="757e46b1" class="cell" data-execution_count="6">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(x.grad, y <span class="op">*</span> x.dot(y).cos())</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="cf">assert</span> torch.allclose(y.grad, x <span class="op">*</span> x.dot(y).cos())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p>At this point, the core idea of automatic differentiation should be clear: the framework does not derive one giant symbolic expression. It only needs to know how to differentiate each primitive operation locally, and then chain these local rules together according to the computation graph.</p>
<p><strong>Looking Inside the Graph: <code>grad_fn</code></strong></p>
<p>If we dig a little deeper, PyTorch exposes part of this backward chain:</p>
<div id="ccddf67c" class="cell" data-execution_count="7">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'z.grad_fn:'</span>, z.grad_fn.name())</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'q.grad_fn:'</span>, q.grad_fn.name())</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x.grad_fn:'</span>, x.grad_fn)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'y.grad_fn:'</span>, y.grad_fn)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>z.grad_fn: SinBackward0
q.grad_fn: DotBackward0
x.grad_fn: None
y.grad_fn: None</code></pre>
</div>
</div>
<p>You will see names such as <code>SinBackward0</code> and <code>DotBackward0</code>. These names can be roughly interpreted as:</p>
<ul>
<li><code>z</code> is not created out of thin air; it is produced by an operator (here, <code>sin</code>).</li>
<li><code>grad_fn</code> is the corresponding backward function object for that operator.</li>
</ul>
<p>During backpropagation, PyTorch starts from the root node and calls each node’s backward function in sequence:</p>
<ul>
<li>When you call <code>z.backward()</code>, PyTorch first invokes <code>SinBackward0</code>, which computes <span class="math inline">\(\frac{\partial z}{\partial q}\)</span>.</li>
<li>It then passes this value to <code>DotBackward0</code>, which computes <span class="math inline">\(\frac{\partial q}{\partial x}\)</span> and <span class="math inline">\(\frac{\partial q}{\partial y}\)</span>.</li>
<li>Finally, the gradients <span class="math inline">\(\frac{\partial z}{\partial x}\)</span> and <span class="math inline">\(\frac{\partial z}{\partial y}\)</span> are accumulated into the leaf tensors.</li>
</ul>
<p>Leaf tensors (<code>x</code>, <code>y</code>) do not have <code>grad_fn</code> because they are the starting points of the graph — there is nothing upstream to differentiate.</p>
<p><strong><code>next_functions</code>: Who is Upstream?</strong></p>
<p>Each backward node also keeps track of its upstream dependencies:</p>
<div id="8a72e740" class="cell" data-execution_count="8">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>node_q <span class="op">=</span> z.grad_fn.next_functions[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>node_x <span class="op">=</span> node_q.next_functions[<span class="dv">0</span>][<span class="dv">0</span>]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>node_y <span class="op">=</span> node_q.next_functions[<span class="dv">1</span>][<span class="dv">0</span>]</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'grad_fn of z.child -&gt; q:'</span>, node_q.name())</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'grad_fn of q.child -&gt; x:'</span>, node_x.name())</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'grad_fn of q.child -&gt; y:'</span>, node_y.name())</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>grad_fn of z.child -&gt; q: DotBackward0
grad_fn of q.child -&gt; x: struct torch::autograd::AccumulateGrad
grad_fn of q.child -&gt; y: struct torch::autograd::AccumulateGrad</code></pre>
</div>
</div>
<p>These connections describe where the backward pass should go next. For example, <code>SinBackward0</code> points to <code>DotBackward0</code>, <code>DotBackward0</code> points to two special nodes called <code>AccumulateGrad</code>.</p>
<p><code>AccumulateGrad</code> is a special node type. Each leaf tensor that requires gradients has an associated <code>AccumulateGrad</code> node. Its job is simple: take the computed gradient and accumulate it into the tensor’s <code>.grad</code> attribute. That is why <code>x.grad</code> and <code>y.grad</code> become populated only after calling <code>backward()</code>.</p>
</section>
<section id="why-non-scalar-outputs-cannot-call-backward-directly" class="level2">
<h2 class="anchored" data-anchor-id="why-non-scalar-outputs-cannot-call-backward-directly">2.1.3 Why Non-Scalar Outputs Cannot Call <code>backward()</code> Directly</h2>
<p>In the previous example, <code>z</code> was a scalar. That is why we could confidently write <code>z.backward()</code>. However, many people will immediately encounter a seemingly unreasonable limitation the first time they replace a scalar output with a vector or matrix:</p>
<div id="8937c3c7" class="cell" data-execution_count="9">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="fl">1.0</span>, <span class="fl">5.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.arange(<span class="fl">5.0</span>, <span class="fl">9.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> torch.outer(x, y)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    Z.backward()  <span class="co"># This will raise an error because z is not a scalar</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> err:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RuntimeError:'</span>, err)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>RuntimeError: grad can be implicitly created only for scalar outputs</code></pre>
</div>
</div>
<p>This is not PyTorch being unreasonable. The reason is that the starting point of backpropagation is no longer unique when the output is not a scalar.</p>
<p>For a scalar <code>z</code>, we usually care about <span class="math inline">\(\frac{\partial z}{\partial x}\)</span> and <span class="math inline">\(\frac{\partial z}{\partial y}\)</span>. The backward pass starts from the output, and the first step is to set <span class="math inline">\(\frac{\partial z}{\partial z} = 1\)</span>. This step is reasonable because the unit gradient for a scalar output is unambiguous: we want to backpropagate along the direction of <code>z</code>.</p>
<p>But what if the output is a vector or matrix <code>Z</code>? What exactly do we want?</p>
<ul>
<li>Do we want the gradient of each element of <code>Z</code> with respect to <code>x</code> and <code>y</code>? That would be a higher-order tensor.</li>
<li>Or do we want some scalar function, such as the sum, mean, or a weighted sum of <code>Z</code>, with respect to <code>x</code> and <code>y</code>?</li>
</ul>
<p>In other words, for non-scalar outputs, backpropagation must first answer one question: from which “direction” do we propagate the gradient?</p>
<p>Mathematically, this “direction” is a tensor <code>v</code> with the same shape as the output:</p>
<p><span class="math display">\[ v = \frac{\partial L}{\partial Z} \]</span></p>
<p>Then PyTorch actually computes a vector–Jacobian product (VJP):</p>
<p><span class="math display">\[ \frac{\partial L}{\partial x} = v^\top \left(\frac{\partial Z}{\partial x}\right) \]</span></p>
<p>For scalar outputs, <code>v</code> is automatically 1 (equivalent to calling <code>Z.backward()</code>, which takes <span class="math inline">\(L\)</span> as <span class="math inline">\(Z\)</span>); for non-scalar outputs, <code>v</code> needs to be provided by ourselves.</p>
<p>There are two ways to write this.</p>
<p>One way is to explicitly pass <code>gradient</code>, indicating the direction along which we want to propagate:</p>
<div id="4ef9af67" class="cell" data-execution_count="10">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="fl">1.0</span>, <span class="fl">5.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.arange(<span class="fl">5.0</span>, <span class="fl">9.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> torch.outer(x, y)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>Z.backward(gradient<span class="op">=</span>torch.ones_like(Z))</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x.grad:'</span>, x.grad)</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'y.grad:'</span>, y.grad)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>x.grad: tensor([26., 26., 26., 26.])
y.grad: tensor([10., 10., 10., 10.])</code></pre>
</div>
</div>
<p>Here, <code>torch.ones_like(Z)</code> means that</p>
<p><span class="math display">\[ \frac{\partial L}{\partial Z_{i,j}} = 1. \]</span></p>
<p>So passing an all-ones gradient is equivalent to defining</p>
<p><span class="math display">\[ L = \sum_{i,j} Z_{i,j} \]</span></p>
<p>and then calling <code>backward()</code>.</p>
<p>Another way is to first convert <code>Z</code> into a scalar and then call <code>backward()</code>:</p>
<div id="974dd203" class="cell" data-execution_count="11">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="fl">1.0</span>, <span class="fl">5.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.arange(<span class="fl">5.0</span>, <span class="fl">9.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> torch.outer(x, y)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>Z <span class="op">=</span> torch.<span class="bu">sum</span>(Z)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>Z.backward()</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'x.grad:'</span>, x.grad)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'y.grad:'</span>, y.grad)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>x.grad: tensor([26., 26., 26., 26.])
y.grad: tensor([10., 10., 10., 10.])</code></pre>
</div>
</div>
<p>In many cases, these two approaches are equivalent. Either we explicitly tell PyTorch along which direction to propagate gradients, or we first reduce the output to a scalar (for example, by summing), so that it implicitly propagates along that scalar direction.</p>
</section>
<section id="higher-order-derivatives-making-the-differentiation-process-itself-part-of-the-computation" class="level2">
<h2 class="anchored" data-anchor-id="higher-order-derivatives-making-the-differentiation-process-itself-part-of-the-computation">2.1.4 Higher-Order Derivatives: Making the Differentiation Process Itself Part of the Computation</h2>
<p>Up to this point, what we have computed are first-order gradients: given a scalar output (or something that can be converted into a scalar) <span class="math inline">\(L\)</span>, we compute <span class="math inline">\(\nabla_x L\)</span> and <span class="math inline">\(\nabla_y L\)</span>. But sometimes we need higher-order information, such as second derivatives (certain directions of the Hessian), curvature, or for use in some regularization terms.</p>
<p>The key point here is: if you want to differentiate a “gradient”, then the process of computing that gradient must itself be differentiable. This is the meaning of <code>create_graph=True</code>. When computing first-order derivatives, we not only compute numerical values, but also record the process of computing those derivatives as a new computation graph.</p>
<p>At this point, many people may ask: why not use <code>backward()</code>? Because the design goal of <code>backward()</code> is training the model. It accumulates gradients into the <code>.grad</code> attributes of leaf tensors and, by default, frees the computation graph to save memory. But when computing higher-order derivatives, we usually want:</p>
<ul>
<li>The gradient to be returned as a tensor (so that it can be further computed).</li>
<li>To optionally retain / construct the computation graph (so that we can differentiate again).</li>
</ul>
<p>Therefore, <code>torch.autograd.grad</code> is more commonly used.</p>
<p>We continue to use the same example: <span class="math inline">\(z = \sin(x \cdot y)\)</span>. We first compute the first-order derivatives <span class="math inline">\(\frac{dz}{dx}\)</span> and <span class="math inline">\(\frac{dz}{dy}\)</span>, and then differentiate these results to see what the second-order derivatives <span class="math inline">\(\frac{d^2 z}{dx^2}\)</span> and <span class="math inline">\(\frac{d^2 z}{dy^2}\)</span> look like.</p>
<div id="c793f16a" class="cell" data-execution_count="12">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">2.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor(<span class="fl">4.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.sin(x <span class="op">*</span> y)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>dzdx, dzdy <span class="op">=</span> torch.autograd.grad(z, (x, y), create_graph<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'dz/dx:'</span>, dzdx)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'dz/dy:'</span>, dzdy)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>dz/dx: tensor(-0.5820, grad_fn=&lt;MulBackward0&gt;)
dz/dy: tensor(-0.2910, grad_fn=&lt;MulBackward0&gt;)</code></pre>
</div>
</div>
<p>The most important line is <code>create_graph=True</code>. Without it, <code>dz/dx</code> and <code>dz/dy</code> would be treated as pure numerical results, and the information about how they were computed would not be retained. Then we would not be able to differentiate them again. The outputs <code>dz/dx</code> and <code>dz/dy</code>both contain a <code>grad_fn</code>, which indicates that they themselves can be differentiated.</p>
<p>When computing higher-order derivatives, we sometimes want to compute gradients with respect to different variables sequentially within the same computation graph. However, after calling <code>backward()</code>, PyTorch by default frees the computation graph to save memory, which prevents further backward passes on the same graph. If we truly need to perform multiple backward passes on the same forward result, we can set <code>retain_graph=True</code> to retain the graph.</p>
<div id="5512cdbc" class="cell" data-execution_count="13">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.tensor(<span class="fl">2.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.tensor(<span class="fl">4.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.sin(x <span class="op">*</span> y)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a>dzdx, dzdy <span class="op">=</span> torch.autograd.grad(z, (x, y), create_graph<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'dz/dx:'</span>, dzdx)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'dz/dy:'</span>, dzdy)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>(d2zdx2,) <span class="op">=</span> torch.autograd.grad(dzdx, x, retain_graph<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>(d2zdy2,) <span class="op">=</span> torch.autograd.grad(dzdy, y)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'d2z/dx2:'</span>, d2zdx2)</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'d2z/dy2:'</span>, d2zdy2)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>dz/dx: tensor(-0.5820, grad_fn=&lt;MulBackward0&gt;)
dz/dy: tensor(-0.2910, grad_fn=&lt;MulBackward0&gt;)
d2z/dx2: tensor(-15.8297)
d2z/dy2: tensor(-3.9574)</code></pre>
</div>
</div>
<p>However, a more common practice is to execute the forward pass again to obtain a new computation graph. <code>retain_graph=True</code> is typically used only when we genuinely need to perform multiple gradient computations on the same graph, such as in experiments with higher-order derivatives or in certain regularization terms.</p>
</section>
<section id="vjp-and-jvp-what-reverse-mode-and-forward-mode-actually-compute" class="level2">
<h2 class="anchored" data-anchor-id="vjp-and-jvp-what-reverse-mode-and-forward-mode-actually-compute">2.1.5 VJP and JVP: What Reverse Mode and Forward Mode Actually Compute</h2>
<p>Up to this point, we have been talking about “computing gradients”. But strictly speaking, most functions in deep learning are not scalar-to-scalar mappings. Instead, they are:</p>
<p><span class="math display">\[ f: \mathbb{R}^n \to \mathbb{R}^m \]</span></p>
<p>Its derivative is a Jacobian matrix:</p>
<p><span class="math display">\[ J = \frac{\partial f}{\partial x} \in \mathbb{R}^{m \times n} \]</span></p>
<p>The real issue is that when both <span class="math inline">\(m,n\)</span> are large, we almost never explicitly construct <span class="math inline">\(J\)</span>. What we actually want, and what the framework computes in practice, is a product involving the Jacobian — either multiplied on the left or on the right.</p>
<section id="vjp-vectorjacobian-product-reverse-mode" class="level3">
<h3 class="anchored" data-anchor-id="vjp-vectorjacobian-product-reverse-mode">2.1.5.1 VJP: Vector–Jacobian Product (Reverse Mode)</h3>
<p>Given an “upstream gradient” vector <span class="math inline">\(v \in \mathbb{R}^m\)</span> (which can be understood as <span class="math inline">\(\frac{\partial L}{\partial f}\)</span>), reverse mode computes:</p>
<p><span class="math display">\[ v^\top J \in \mathbb{R}^n \]</span></p>
<p>This is called the <strong>vector–Jacobian product (VJP)</strong>.</p>
<p>In the language of training:</p>
<ul>
<li>We have a scalar <code>loss</code>: <span class="math inline">\(L = \mathcal{L}(f(x))\)</span></li>
<li>An upstream gradient: <span class="math inline">\(v = \frac{\partial L}{\partial f}\)</span></li>
<li>Backpropagation computes: <span class="math inline">\(\frac{\partial L}{\partial x} = v^\top \frac{\partial f}{\partial x}\)</span></li>
</ul>
<p>So when we call <code>backward()</code> in practice, what is actually being computed is a special case of VJP.</p>
<div id="aa45c237" class="cell" data-execution_count="14">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> vjp_func(x: torch.Tensor, y: torch.Tensor):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.sin(torch.dot(x, y))</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="fl">1.0</span>, <span class="fl">5.0</span>)</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.arange(<span class="fl">5.0</span>, <span class="fl">9.0</span>)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> AF.vjp(vjp_func, (x, y))</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'func(x,y):'</span>, out[<span class="dv">0</span>])</span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'VJP output:'</span>, out[<span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>func(x,y): tensor(0.7739)
VJP output: (tensor([3.1666, 3.7999, 4.4332, 5.0666]), tensor([0.6333, 1.2666, 1.9000, 2.5333]))</code></pre>
</div>
</div>
</section>
<section id="jvp-jacobianvector-product-forward-mode" class="level3">
<h3 class="anchored" data-anchor-id="jvp-jacobianvector-product-forward-mode">2.1.5.2 JVP: Jacobian–Vector Product (Forward Mode)</h3>
<p>Forward mode is the opposite. Given an input direction <span class="math inline">\(u \in \mathbb{R}^n\)</span>, it computes:</p>
<p><span class="math display">\[ Ju \in \mathbb{R}^m \]</span></p>
<p>This is called the <strong>Jacobian–vector product (JVP)</strong>.</p>
<p>Intuitively, it answers the following question: if we make a small perturbation in the input space along direction <span class="math inline">\(u\)</span>, along which direction will the output change? This is common in sensitivity analysis, implicit layers, certain second-order methods, and some physics or scientific computing settings.</p>
<div id="f1a51552" class="cell" data-execution_count="15">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> jvp_func(a: torch.Tensor, b: torch.Tensor):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> torch.sin(torch.dot(a, b))</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="fl">1.0</span>, <span class="fl">5.0</span>)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.arange(<span class="fl">5.0</span>, <span class="fl">9.0</span>)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>v_x <span class="op">=</span> torch.full_like(x, <span class="fl">0.1</span>)</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>v_y <span class="op">=</span> torch.full_like(y, <span class="fl">0.2</span>)</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>out <span class="op">=</span> AF.jvp(jvp_func, (x, y), (v_x, v_y))</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'func(x,y):'</span>, out[<span class="dv">0</span>])</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'JVP output:'</span>, out[<span class="dv">1</span>])</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>func(x,y): tensor(0.7739)
JVP output: tensor(2.9133)</code></pre>
</div>
</div>
</section>
<section id="why-vjp-is-more-common-in-deep-learning" class="level3">
<h3 class="anchored" data-anchor-id="why-vjp-is-more-common-in-deep-learning">2.1.5.3 Why VJP Is More Common in Deep Learning</h3>
<p>This is not about which method is “more advanced,” but about scale matching.</p>
<ul>
<li>In deep learning training, <span class="math inline">\(n\)</span> is usually the parameter dimension (millions or billions).</li>
<li><span class="math inline">\(m\)</span> is usually the output dimension (often a scalar).</li>
<li>What we want is <span class="math inline">\(\nabla_x L \in \mathbb{R}^n\)</span>.</li>
</ul>
<p>The computational cost of VJP is roughly on the order of one backward pass. It is suitable when <span class="math inline">\(n\)</span> is large but the output is scalar or low-dimensional. JVP is more suitable when the input dimension is relatively small, but we care about how the output changes along certain directions.</p>
<p>Therefore, a common rule of thumb is:</p>
<ul>
<li>If the output is scalar or low-dimensional and the input dimension is large, reverse mode (VJP) is more appropriate.</li>
<li>If the input dimension is relatively small and the output dimension is large, forward mode (JVP) may be more appropriate.</li>
</ul>
</section>
</section>
<section id="common-errors-in-backpropagation" class="level2">
<h2 class="anchored" data-anchor-id="common-errors-in-backpropagation">2.1.6 Common Errors in Backpropagation</h2>
<div id="578f62d0" class="cell" data-execution_count="16">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.arange(<span class="fl">1.0</span>, <span class="fl">5.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> torch.arange(<span class="fl">5.0</span>, <span class="fl">9.0</span>, requires_grad<span class="op">=</span><span class="va">True</span>)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Calling <code>backward()</code> multiple times</strong></p>
<p>Calling <code>backward()</code> multiple times on the same computation graph will cause an error. After the first backward pass finishes, PyTorch frees the intermediate tensors that were saved only for backpropagation, in order to save memory. Therefore, when we try to traverse the same graph a second time, we find that the “markers” have already been cleared. If multiple gradient computations are needed, you can set <code>retain_graph=True</code> in the first call.</p>
<div id="dfa70386" class="cell" data-execution_count="17">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.sin(torch.dot(x, y))</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>z.backward()</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    z.backward()  <span class="co"># This will raise an error because gradients are already computed</span></span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> err:</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RuntimeError:'</span>, err)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.</code></pre>
</div>
</div>
<div id="f0d93e2b" class="cell" data-execution_count="18">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.sin(torch.dot(x, y))</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>z.backward(retain_graph<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>z.backward()  <span class="co"># This works because we retained the graph</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>
<p><strong>Trying to access the gradient of intermediate nodes</strong></p>
<p>Only leaf nodes (that is, the original tensors created by the user) store gradient information. Gradients of intermediate nodes are not stored. If every intermediate tensor stored its gradient, memory usage would increase dramatically. Moreover, during training we only need parameter gradients, not gradients of all intermediate values. Therefore, attempting to access the <code>.grad</code> attribute of intermediate tensors will return <code>None</code> and may trigger a <code>UserWarning</code>. If you need to retain the gradient of an intermediate node, you can call <code>retain_grad()</code> on that tensor when it is created.</p>
<div id="5819a95f" class="cell" data-execution_count="19">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> torch.dot(x, y)</span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.sin(q)</span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a>z.backward()</span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> warnings.catch_warnings(record<span class="op">=</span><span class="va">True</span>) <span class="im">as</span> w:</span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'q.grad:'</span>, q.grad)</span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="bu">len</span>(w) <span class="op">&gt;</span> <span class="dv">0</span>:</span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> warn <span class="kw">in</span> w:</span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="st">'UserWarning:'</span>, warn.message)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>q.grad: None
UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at C:\actions-runner\_work\pytorch\pytorch\pytorch\build\aten\src\ATen/core/TensorBody.h:497.)</code></pre>
</div>
</div>
<div id="f8849d9a" class="cell" data-execution_count="20">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>q <span class="op">=</span> torch.dot(x, y)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>q.retain_grad()</span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.sin(q)</span>
<span id="cb34-4"><a href="#cb34-4" aria-hidden="true" tabindex="-1"></a>z.backward()</span>
<span id="cb34-5"><a href="#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'q.grad after retain_grad:'</span>, q.grad)  <span class="co"># Now q.grad is available</span></span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>q.grad after retain_grad: tensor(0.6333)</code></pre>
</div>
</div>
<p><strong>Using in-place operations</strong></p>
<p>In PyTorch, operations with a trailing underscore, such as <code>x.add_(1)</code> or <code>x.relu_()</code>, modify the tensor in place. They do not create a new tensor, but directly modify the memory of <code>x</code>. This may appear convenient, but during backpropagation, PyTorch often needs certain intermediate values from the forward pass. If those values are modified in place after the forward pass, the backward computation may lose the information required to compute gradients correctly. Therefore, during backpropagation, it is recommended to avoid in-place operations, or ensure that they do not modify intermediate variables required for gradient computation.</p>
<div id="42c36582" class="cell" data-execution_count="21">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.dot(x, y)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="cf">try</span>:</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>    x.relu_()</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a><span class="cf">except</span> <span class="pp">RuntimeError</span> <span class="im">as</span> err:</span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'RuntimeError:'</span>, err)</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output cell-output-stdout">
<pre><code>RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.</code></pre>
</div>
</div>
<div id="9571ed90" class="cell" data-execution_count="22">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>z <span class="op">=</span> torch.dot(x, y)</span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> torch.relu(x)</span>
<span id="cb38-3"><a href="#cb38-3" aria-hidden="true" tabindex="-1"></a>z.backward()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
</div>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-reuse"><h2 class="anchored quarto-appendix-heading">Reuse</h2><div class="quarto-appendix-contents"><div><a rel="license" href="https://creativecommons.org/licenses/by-nc/4.0/">CC BY-NC 4.0</a></div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("https:\/\/jshn9515\.github\.io\/deep-learning-notes\/");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




<footer class="footer"><div class="nav-footer"><div class="nav-footer-center"><div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/jshn9515/deep-learning-notes/blob/main/en/ch2-pytorch-introduction/ch2.1-automatic-differentiation.ipynb" class="toc-action"><i class="bi bi-github"></i>View source</a></li><li><a href="https://github.com/jshn9515/deep-learning-notes/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li><li><a href="https://github.dev/jshn9515/deep-learning-notes/blob/main/en/ch2-pytorch-introduction/ch2.1-automatic-differentiation.ipynb" class="toc-action"><i class="bi empty"></i>Edit this page</a></li></ul></div></div></div></footer></body></html>