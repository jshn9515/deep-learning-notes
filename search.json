[
  {
    "objectID": "zh/index.html",
    "href": "zh/index.html",
    "title": "深度学习笔记",
    "section": "",
    "text": "深度学习笔记\n欢迎来到我的深度学习笔记。\n\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html",
    "href": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html",
    "title": "Chapter 2.1 PyTorch 中的自动微分",
    "section": "",
    "text": "在 1.3 节里，我们把计算图当成一条“责任链”：损失函数为什么是这个值，沿着链条往回追，就能追到每个参数到底“负了多少责任”。这一节我们换一个更工程的视角：框架是怎么把这条责任链自动搭起来，并且在需要的时候把梯度算出来的？\n先把问题说得更直白一点：训练时我们要的是梯度，但我们手里只有一堆代码：加法、乘法、卷积、激活函数…。这些操作在前向传播里一行行执行，最后吐出一个 loss。那么，梯度从哪来？难道框架真的去推导一个巨大的符号表达式吗？\n当然不是。深度学习框架做的事情更像是：\n理解这套机制很关键。它不仅解释了“梯度是怎么来的”，还会直接影响我们后面遇到的许多现象：比如梯度为什么会累积？为什么中间变量默认没有 .grad 属性？为什么有些操作会切断梯度链条？以及显存与计算之间为什么总要做权衡。\nimport torch\nimport torch.autograd.functional as AF",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.1 PyTorch 中的自动微分"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#计算图不是画出来的是跑出来的",
    "href": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#计算图不是画出来的是跑出来的",
    "title": "Chapter 2.1 PyTorch 中的自动微分",
    "section": "2.1.1 计算图不是画出来的，是跑出来的",
    "text": "2.1.1 计算图不是画出来的，是跑出来的\n理解 PyTorch 的自动微分，最好的方式不是先背概念，而是先观察一件事：你只是在做前向计算，但计算图会在运行过程中自动搭建出来。\n假设我们有这样一个简单的函数：\n\\[ z = \\sin(x \\cdot y) \\]\n我们可以把它拆解成几个基本的运算步骤：\n\n计算向量内积：\\(q = x \\cdot y\\)\n计算正弦函数：\\(z = \\sin(q)\\)\n\n然后，我们告诉 PyTorch，在接下来的计算中，我们希望得到 z 关于 x 和 y 的梯度。\n\nx = torch.arange(1.0, 5.0, requires_grad=True)\ny = torch.arange(5.0, 9.0, requires_grad=True)\n\n这里的 requires_grad=True 可以理解成一种声明：这些变量需要被“追责”。之后只要某个结果是由它们参与计算得到的，它就会自动带上可导属性，并在背后记录“我是谁算出来的，依赖了谁”。\n现在做两步普通的前向计算：先算点积，再取正弦。\n\nq = torch.dot(x, y)\nz = torch.sin(q)\nprint('z.requires_grad:', z.requires_grad)\n\nz.requires_grad: True\n\n\n到这里你看到的依然只是数值计算，但 PyTorch 已经做了两件事：\n\nz 会自动变成需要梯度的结果（因为它依赖了需要梯度的 x 和 y）。\nq 和 z 的产生过程会被记录下来：z 由 sin 得到，q 由 dot 得到，而 q 又依赖 x 和 y。\n\n先别急着管计算图长什么样。我们先看一个更直观的现象：在你调用反向传播之前，梯度并不会凭空出现。\n\nprint('x.grad:', x.grad)\nprint('y.grad:', y.grad)\n\nx.grad: None\ny.grad: None\n\n\n这里是 None，而不是 0。原因也很简单：梯度是一种反向回溯的产物，只有当你明确发起回溯（比如调用 backward()）时，PyTorch 才会沿着刚才记录的依赖关系，把梯度算出来并写回到叶子节点上。如果不调用，PyTorch 就不会去算梯度，自然也不会给你填上数值。\n接下来我们就做这件事：从 z 开始反向传播，看看 .grad 是如何出现的，以及它和我们手算的结果是否一致。",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.1 PyTorch 中的自动微分"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#backward-到底做了什么从输出往回查账",
    "href": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#backward-到底做了什么从输出往回查账",
    "title": "Chapter 2.1 PyTorch 中的自动微分",
    "section": "2.1.2 backward 到底做了什么：从输出往回“查账”",
    "text": "2.1.2 backward 到底做了什么：从输出往回“查账”\n上一节我们只做了前向计算，但 PyTorch 已经把依赖关系悄悄记录好了。现在我们真正关心的是：当你调用 backward() 时，框架究竟做了什么？算出来的梯度又是否可信？\n还是沿用同一个例子：\n\\[ q = x^\\top y, \\quad z = \\sin(q) \\]\n如果我们手算梯度，我们就会得到：\n\\[ \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial q} \\cdot \\frac{\\partial q}{\\partial x} = \\cos(q) \\cdot y \\] \\[ \\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial q} \\cdot \\frac{\\partial q}{\\partial y} = \\cos(q) \\cdot x \\]\n好的，现在让 PyTorch 来算。我们直接从输出 z 发起回溯：\n\nz.backward()\nprint('x.grad:', x.grad)\nprint('y.grad:', y.grad)\n\nx.grad: tensor([3.1666, 3.7999, 4.4332, 5.0666])\ny.grad: tensor([0.6333, 1.2666, 1.9000, 2.5333])\n\n\n此时 .grad 不再是 None，梯度已经被写回到了 x、y 这两个叶子节点上。直觉上你可以这样理解 backward()：\n\n以 z 为起点，默认认为 \\(\\frac{\\partial z}{\\partial z} = 1\\)；\n然后沿着前向传播时记下来的依赖链往回走；\n每走过一个算子节点，就用这个算子的局部求导规则把梯度继续往上游传递。\n\n我们可以把它和手算结果对齐。比如：\n\nassert torch.allclose(x.grad, y * x.dot(y).cos())\nassert torch.allclose(y.grad, x * x.dot(y).cos())\n\n到这里，自动微分的核心逻辑其实已经很清楚了。深度学习框架并不需要推导一个巨大的全局导数公式，它只需要知道每一步怎么求导，然后把这些局部规则按计算图的结构串起来。\n如果再深入一点，其实 PyTorch 也把这条回溯链暴露了一部分给我们。比如：\n\nprint('z.grad_fn:', z.grad_fn.name())\nprint('q.grad_fn:', q.grad_fn.name())\nprint('x.grad_fn:', x.grad_fn)\nprint('y.grad_fn:', y.grad_fn)\n\nz.grad_fn: SinBackward0\nq.grad_fn: DotBackward0\nx.grad_fn: None\ny.grad_fn: None\n\n\n我们通常会看到类似 SinBackward0 这样带有 Backward 的名字。它的含义可以粗略理解为：\n\nz 不是凭空来的，它是某个算子（这里是 sin）产生的结果；\ngrad_fn 就是这个算子在反向传播时对应的梯度函数对象。\n\n在计算反向传播时，PyTorch 从根节点开始，依次调用每个节点的导数算子，计算出各个输入变量的梯度，直到到达输入节点为止。例如，当我们调用 z.backward() 时，PyTorch 会首先调用 z 节点的导数算子 SinBackward0，计算出 \\(\\frac{\\partial z}{\\partial q}\\)，然后将该值传递给 q 节点的导数算子 DotBackward0，计算出 \\(\\frac{\\partial q}{\\partial x}\\) 和 \\(\\frac{\\partial q}{\\partial y}\\)，最终得到 \\(\\frac{\\partial z}{\\partial x}\\) 和 \\(\\frac{\\partial z}{\\partial y}\\)。叶子节点（如 x 和 y）没有导数算子，因为它们是计算图的起点，不需要进一步计算梯度。\n更关键的是，grad_fn.next_functions 会指向它的上游依赖：\n\nnode_q = z.grad_fn.next_functions[0][0]\nnode_x = node_q.next_functions[0][0]\nnode_y = node_q.next_functions[1][0]\nprint('grad_fn of z.child -&gt; q:', node_q.name())\nprint('grad_fn of q.child -&gt; x:', node_x.name())\nprint('grad_fn of q.child -&gt; y:', node_y.name())\n\ngrad_fn of z.child -&gt; q: DotBackward0\ngrad_fn of q.child -&gt; x: struct torch::autograd::AccumulateGrad\ngrad_fn of q.child -&gt; y: struct torch::autograd::AccumulateGrad\n\n\n它们描述的是，为了计算 z 的梯度，反向传播接下来应该去找谁、沿着哪些输入回溯。例如，在 SinBackward0 节点中，next_functions 会指向 DotBackward0 节点，因为 SinBackward0 的输入是 q，而 q 是通过 DotBackward0 计算得到的。同样地，在 DotBackward0 节点中，next_functions 会指向输入节点 x 和 y。AccumulateGrad 是一个特殊的节点类型，每个需要梯度的叶子节点前都会有一个对应的 AccumulateGrad 节点，负责把得到的梯度累加到叶子节点的 .grad 属性中。这也就是为什么 x.grad、y.grad 最终会在调用 backward() 后出现。",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.1 PyTorch 中的自动微分"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#为什么非标量不能直接-backward",
    "href": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#为什么非标量不能直接-backward",
    "title": "Chapter 2.1 PyTorch 中的自动微分",
    "section": "2.1.3 为什么非标量不能直接 backward",
    "text": "2.1.3 为什么非标量不能直接 backward\n上面的例子里，z 是一个标量，所以我们可以理直气壮地写 z.backward()。相信很多人第一次换成输出是向量或者矩阵时，会立刻撞到 PyTorch 的一条看起来很不讲理的限制：\n\nx = torch.arange(1.0, 5.0, requires_grad=True)\ny = torch.arange(5.0, 9.0, requires_grad=True)\nZ = torch.outer(x, y)\ntry:\n    Z.backward()  # This will raise an error because z is not a scalar\nexcept RuntimeError as err:\n    print('RuntimeError:', err)\n\nRuntimeError: grad can be implicitly created only for scalar outputs\n\n\n这不是 PyTorch 小气，而是反向传播的起点在非标量情况下不再唯一。\n对标量 z，我们通常关心的是 \\(\\frac{\\partial z}{\\partial x}\\) 和 \\(\\frac{\\partial z}{\\partial y}\\)。反向传播从输出出发，第一步就是设定 \\(\\frac{\\partial z}{\\partial z} = 1\\)。这一步之所以合理，是因为标量输出的单位梯度没有歧义：我们就是要沿着 z 这个方向往回传。\n但是，如果输出是向量或者矩阵 Z 呢？我们到底想要什么？\n\n是想要 Z 的每一个元素对 x 和 y 的梯度吗？那会是一个更高阶的张量。\n还是想要某个标量函数，比如 Z 的和、均值、某个加权和，对 x 和 y 的梯度？\n\n也就是说，对非标量输出，反向传播必须先回答一句话：我们打算从哪个“方向”把梯度回传？\n在数学上，这个“方向”就是一个与输出同形状的张量 v，表示从上游传下来的梯度：\n\\[ v = \\frac{\\partial L}{\\partial Z} \\]\n然后 PyTorch 实际计算的是向量-雅可比积（VJP）：\n\\[ \\frac{\\partial L}{\\partial x} = v^\\top \\left(\\frac{\\partial Z}{\\partial x}\\right) \\]\n对于标量输出，v 自动为 1（等价于调用 Z.backward()，即把 \\(L\\) 取为 \\(Z\\)）；对于非标量输出，v 需要我们自己提供。\n这里就有两种写法。\n一种写法是，我们显式传入 gradient，表示我们想要从哪个方向回传梯度：\n\nx = torch.arange(1.0, 5.0, requires_grad=True)\ny = torch.arange(5.0, 9.0, requires_grad=True)\nZ = torch.outer(x, y)\nZ.backward(gradient=torch.ones_like(Z))\nprint('x.grad:', x.grad)\nprint('y.grad:', y.grad)\n\nx.grad: tensor([26., 26., 26., 26.])\ny.grad: tensor([10., 10., 10., 10.])\n\n\n这里 torch.ones_like(Z) 就是告诉 PyTorch，我想让 \\(L = \\sum_{i,j} Z_{i,j}\\)，因为\n\\[ \\frac{\\partial L}{\\partial Z_{i,j}} = 1 \\]\n所以传一个全 1 的梯度，就等价于“对所有元素求和后再 backward”。\n还有另外一种写法，就是先把 Z 变成一个标量，再对这个标量调用 backward()：\n\nx = torch.arange(1.0, 5.0, requires_grad=True)\ny = torch.arange(5.0, 9.0, requires_grad=True)\nZ = torch.outer(x, y)\nZ = torch.sum(Z)\nZ.backward()\nprint('x.grad:', x.grad)\nprint('y.grad:', y.grad)\n\nx.grad: tensor([26., 26., 26., 26.])\ny.grad: tensor([10., 10., 10., 10.])\n\n\n这两种写法在很多情况下是等价的。要么我们显式告诉 PyTorch 从哪个方向回传梯度，要么我们先把输出变成一个标量（比如求和），让它自己默认从这个标量的方向回传梯度。",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.1 PyTorch 中的自动微分"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#高阶导数让求导过程也变成计算的一部分",
    "href": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#高阶导数让求导过程也变成计算的一部分",
    "title": "Chapter 2.1 PyTorch 中的自动微分",
    "section": "2.1.4 高阶导数：让求导过程也变成计算的一部分",
    "text": "2.1.4 高阶导数：让求导过程也变成计算的一部分\n到目前为止，我们做的都是一阶梯度：给定一个标量输出（或者可以转换成标量输出）\\(L\\)，求 \\(\\nabla_x L\\)，\\(\\nabla_y L\\)。但有时候我们会需要更高阶的信息，比如二阶导数（Hessian 的某些方向）、曲率、或者用在一些正则项里。\n那么这件事的关键点在于：如果你想对“梯度”再求导，那么“求梯度这件事”本身也必须是可微的。这就是 create_graph=True 的含义。在计算一阶导数时，不仅算出数值，还要把“算出这个导数的过程”记录成新的计算图。\n可能这时候很多人就会有疑惑，为什么不用 backward() 呢？因为 backward() 的设计目标是训练模型：我们把梯度累积进叶子张量的 .grad 属性中，并且默认释放图来节省内存。但是，在做高阶导时，我们更希望：\n\n梯度作为一个张量返回（方便继续算）\n必要时保留 / 构建计算图（方便再求导）\n\n因此更常用的是 torch.autograd.grad。\n我们还是用上面的例子：\\(z = \\sin(x \\cdot y)\\)。我们先求一阶导数 \\(\\frac{dz}{dx}\\) 和 \\(\\frac{dz}{dy}\\)，然后再对这个结果求导，看看二阶导数 \\(\\frac{d^2 z}{dx^2}\\) 和 \\(\\frac{d^2 z}{dy^2}\\) 是什么样的。\n\nx = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(4.0, requires_grad=True)\nz = torch.sin(x * y)\n\ndzdx, dzdy = torch.autograd.grad(z, (x, y), create_graph=True)\nprint('dz/dx:', dzdx)\nprint('dz/dy:', dzdy)\n\ndz/dx: tensor(-0.5820, grad_fn=&lt;MulBackward0&gt;)\ndz/dy: tensor(-0.2910, grad_fn=&lt;MulBackward0&gt;)\n\n\n这里最重要的一行是 create_graph=True。如果没有它，dz/dx 和 dz/dy 会被当成纯数值结果，不再保留它是怎么得到的。那我们就没法再对它求导。dz/dx 和 dz/dy 的输出都包含了一个 grad_fn，说明他们允许自身被求导。\n在计算高阶导数时，我们有时候希望在同一个计算图中前后对不同变量分别求导。但是，PyTorch 在调用一次 backward() 后默认会释放计算图来节省内存，这就导致我们无法在同一个图里连续求导。如果我们确实需要在同一次前向结果上做多次回溯，可以通过设置 retain_graph=True 来保留图：\n\nx = torch.tensor(2.0, requires_grad=True)\ny = torch.tensor(4.0, requires_grad=True)\nz = torch.sin(x * y)\n\ndzdx, dzdy = torch.autograd.grad(z, (x, y), create_graph=True)\nprint('dz/dx:', dzdx)\nprint('dz/dy:', dzdy)\n\n(d2zdx2,) = torch.autograd.grad(dzdx, x, retain_graph=True)\n(d2zdy2,) = torch.autograd.grad(dzdy, y)\nprint('d2z/dx2:', d2zdx2)\nprint('d2z/dy2:', d2zdy2)\n\ndz/dx: tensor(-0.5820, grad_fn=&lt;MulBackward0&gt;)\ndz/dy: tensor(-0.2910, grad_fn=&lt;MulBackward0&gt;)\nd2z/dx2: tensor(-15.8297)\nd2z/dy2: tensor(-3.9574)\n\n\n不过更常见的做法是，重新执行一次前向传播来得到一张新的计算图。retain_graph=True 通常是当我们确实要在同一个计算图上做多次梯度计算时才用，比如高阶导数实验或者某些正则项的计算。",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.1 PyTorch 中的自动微分"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#vjp-和-jvp反向模式与正向模式到底在算什么",
    "href": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#vjp-和-jvp反向模式与正向模式到底在算什么",
    "title": "Chapter 2.1 PyTorch 中的自动微分",
    "section": "2.1.5 VJP 和 JVP：反向模式与正向模式到底在算什么",
    "text": "2.1.5 VJP 和 JVP：反向模式与正向模式到底在算什么\n到目前为止我们一直在说“求梯度”。但严格来说，深度学习里绝大多数函数并不是从标量到标量，而是：\n\\[ f: \\mathbb{R}^n \\to \\mathbb{R}^m \\]\n它的导数是一个雅可比矩阵（Jacobian）：\n\\[ J = \\frac{\\partial f}{\\partial x} \\in \\mathbb{R}^{m \\times n} \\]\n真正的问题是，当 \\(m,n\\) 都很大时，我们几乎从来不会显式构造 \\(J\\)。我们真正想要的，框架实际计算的是 Jacobian 的乘积，要么乘在左边，要么乘在右边。\n\n2.1.5.1 VJP：向量-雅可比积（反向模式）\n给定“上游梯度”向量 \\(v \\in \\mathbb{R}^m\\)（可以理解为 \\(\\frac{\\partial L}{\\partial f}\\)），反向模式计算的是：\n\\[ v^\\top J \\in \\mathbb{R}^n \\]\n这就是 VJP（vector-Jacobian product）。\n把它翻译成训练时的语言就更熟悉了：\n\n我们有一个标量 loss：\\(L = \\mathcal{L}(f(x))\\)\n一个上游梯度：\\(v = \\frac{\\partial L}{\\partial f}\\)\n进行反向传播：\\(\\frac{\\partial L}{\\partial x} = v^\\top \\frac{\\partial f}{\\partial x}\\)\n\n所以，平时我们调用 backward()，实际上就是在计算一个特殊的 VJP。\n\ndef vjp_func(x: torch.Tensor, y: torch.Tensor):\n    return torch.sin(torch.dot(x, y))\n\n\nx = torch.arange(1.0, 5.0)\ny = torch.arange(5.0, 9.0)\nout = AF.vjp(vjp_func, (x, y))\nprint('func(x,y):', out[0])\nprint('VJP output:', out[1])\n\nfunc(x,y): tensor(0.7739)\nVJP output: (tensor([3.1666, 3.7999, 4.4332, 5.0666]), tensor([0.6333, 1.2666, 1.9000, 2.5333]))\n\n\n\n\n2.1.5.2 JVP：雅可比-向量积（正向模式）\n正向模式则相反：给定一个输入方向 \\(u \\in \\mathbb{R}^n\\)，计算：\n\\[ Ju \\in \\mathbb{R}^m \\]\n这就是 JVP（Jacobian-vector product）。从直觉上，它回答的问题是：如果我们在输入空间里沿某个方向 \\(u\\) 做一个微小的扰动，输出会沿着哪个方向变化？这在做敏感性分析、隐式层、某些二阶方法、以及一些物理/科学计算中非常常见。\n\ndef jvp_func(a: torch.Tensor, b: torch.Tensor):\n    return torch.sin(torch.dot(a, b))\n\n\nx = torch.arange(1.0, 5.0)\ny = torch.arange(5.0, 9.0)\nv_x = torch.full_like(x, 0.1)\nv_y = torch.full_like(y, 0.2)\nout = AF.jvp(jvp_func, (x, y), (v_x, v_y))\nprint('func(x,y):', out[0])\nprint('JVP output:', out[1])\n\nfunc(x,y): tensor(0.7739)\nJVP output: tensor(2.9133)\n\n\n\n\n2.1.5.3 为什么深度学习里更常见的是 VJP\n这个问题不是“谁更高级”，而是”规模匹配”。\n\n在深度学习训练中，通常 \\(n\\) 是参数维度（百万/亿级），\\(m\\) 是输出维度（通常是一个标量）\n我们真正想要的是 \\(\\nabla L \\in \\mathbb{R}^n\\)\n\nVJP 的复杂度大致和“一次反向传播”同量级，适合 \\(n\\) 很大但输出是标量/低维的场景。JVP 更适合输入维度相对小，但我们关心输出方向变化的场景。所以，我们会看到一个很经典的判断：如果输出是标量或低维向量，而且输入维度很大，那么反向模式（VJP）更合适；如果输入维度相对较小，输出维度很大，那么正向模式（JVP）可能更合适。",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.1 PyTorch 中的自动微分"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#反向传播中的常见错误",
    "href": "zh/ch2-pytorch-introduction/ch2.1-automatic-differentiation.html#反向传播中的常见错误",
    "title": "Chapter 2.1 PyTorch 中的自动微分",
    "section": "2.1.6 反向传播中的常见错误",
    "text": "2.1.6 反向传播中的常见错误\n\nx = torch.arange(1.0, 5.0, requires_grad=True)\ny = torch.arange(5.0, 9.0, requires_grad=True)\n\n\n重复调用 backward()：在同一个计算图上多次调用 backward() 会导致错误。PyTorch 在第一次反向传播结束后，会把这张图里“只为反向传播服务的中间变量”释放掉，以节省显存。所以当我们第二次再沿着同一张图回溯，就会发现“路标”已经被清理了。如果需要多次计算梯度，可以在第一次调用时设置 retain_graph=True。\n\n\nz = torch.sin(torch.dot(x, y))\nz.backward()\ntry:\n    z.backward()  # This will raise an error because gradients are already computed\nexcept RuntimeError as err:\n    print('RuntimeError:', err)\n\nRuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n\n\n\nz = torch.sin(torch.dot(x, y))\nz.backward(retain_graph=True)\nz.backward()  # This works because we retained the graph\n\n\n尝试访问中间节点的梯度：只有叶子节点（即最初创建的变量）会存储梯度信息。中间节点的梯度不会被存储，因为如果每个中间变量都存梯度，显存会直接爆炸，而且训练真正需要的是参数梯度，而不是所有中间量的梯度。因此尝试访问它们的 .grad 属性会返回 None，并引发 UserWarning。如果需要保留中间节点的梯度，可以在创建这些节点时设置 q.retain_grad()。\n\n\nimport warnings\n\nq = torch.dot(x, y)\nz = torch.sin(q)\nz.backward()\n\nwith warnings.catch_warnings(record=True) as w:\n    print('q.grad:', q.grad)\n    if len(w) &gt; 0:\n        for warn in w:\n            print('UserWarning:', warn.message)\n\nq.grad: None\nUserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:497.)\n\n\n\nq = torch.dot(x, y)\nq.retain_grad()\nz = torch.sin(q)\nz.backward()\nprint('q.grad after retain_grad:', q.grad)  # Now q.grad is available\n\nq.grad after retain_grad: tensor(0.6333)\n\n\n\n使用原地操作：PyTorch 里像 x.add_(1)、x.relu_() 这种带下划线的操作，表示原地修改张量。不创建新张量，而是直接改 x 自己的内存。这在直觉上很省事，但在反向传播往往需要用到前向传播时的某些中间值。如果这些值在前向之后被我们就地改掉，那反向传播就可能失去计算梯度所需的信息。因此，在反向传播过程中，尽量避免使用原地操作，或者确保它们不会修改反向传播需要的中间变量。\n\n\nz = torch.dot(x, y)\ntry:\n    x.relu_()\nexcept RuntimeError as err:\n    print('RuntimeError:', err)\n\nRuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n\n\n\nz = torch.dot(x, y)\nx = torch.relu(x)\nz.backward()",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.1 PyTorch 中的自动微分"
    ]
  },
  {
    "objectID": "zh/ch1-introduction/ch1.3-computation-graph.html",
    "href": "zh/ch1-introduction/ch1.3-computation-graph.html",
    "title": "Chapter 1.3 前向传播、反向传播与计算图",
    "section": "",
    "text": "计算图（Computational Graph） 是深度学习中最核心的概念之一。理解了计算图，我们就能理解神经网络是如何计算输出的，以及它是如何根据误差自动调整参数的。\n在前面的章节中，我们已经介绍了损失函数。损失函数衡量了模型的误差，而深度学习的目标，就是通过调整参数，使损失函数尽可能小。但要做到这一点，我们必须回答一个关键问题：每个参数是如何影响损失函数的？\n计算图提供了这个问题的答案。它清晰地记录了从输入到损失函数的整个计算过程，使我们能够系统地分析每个变量的作用。\n但是，计算图的概念对于初学者来说可能有些抽象。因此，在本章中，我们将通过一个简单例子，逐步介绍梯度、计算图、前向传播和反向传播之间的关系。",
    "crumbs": [
      "中文",
      "深度学习简介",
      "Chapter 1.3 前向传播、反向传播与计算图"
    ]
  },
  {
    "objectID": "zh/ch1-introduction/ch1.3-computation-graph.html#梯度",
    "href": "zh/ch1-introduction/ch1.3-computation-graph.html#梯度",
    "title": "Chapter 1.3 前向传播、反向传播与计算图",
    "section": "1.3.1 梯度",
    "text": "1.3.1 梯度\n在深度学习里，我们要做的事情很简单：让损失函数 \\(L\\) 尽可能小。但是问题是，神经网络模型有成千上万的参数，我们不可能一个一个去试。这时候就需要有一个工具，来帮忙解决两个问题：\n\n该调哪个参数？\n每个参数该往哪个方向调？该调多少？\n\n而 梯度（gradient） 就是用来回答这两个问题的。\n相信大家都学过偏导数。如果我们有一个函数 \\(f(x, y)\\)，当我们想知道“只改变 \\(x\\) 会让 \\(f\\) 怎么变”，我们就看 \\(\\frac{\\partial f}{\\partial x}\\)；当我们想知道“只改变 \\(y\\) 会让 \\(f\\) 怎么变”，我们就看 \\(\\frac{\\partial f}{\\partial y}\\)。偏导数的大小可以理解为一种“敏感度”。偏导数的绝对值越大，说明这个参数的轻微变化就会让 \\(f\\) 变化很大；反之，偏导数的绝对值越小，说明这个参数的轻微变化对 \\(f\\) 的影响就越小。\n当参数不止一个时，我们就可以把这些参数的偏导数“打包”，形成一个向量，这个向量就是梯度。对于函数 \\(f(x, y)\\)，它的梯度就是 \\(\\nabla f = \\left( \\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y} \\right)\\)。梯度是一个向量，我们可以把它想象成一个箭头。箭头的方向表示函数值增加最快的方向（不是减少！），箭头的长度表示函数值增加的速度。\n更一般地，如果模型参数是一个高维向量：\n\\[\\theta = (w_1, w_2, ..., w_n)\\]\n假设损失函数是 \\(L(\\theta)\\)，那么对应的梯度就是：\n\\[\\nabla L = \\left( \\frac{\\partial L}{\\partial w_1}, \\frac{\\partial L}{\\partial w_2}, ..., \\frac{\\partial L}{\\partial w_n} \\right)\\]\n梯度的每一个分量都对应一个参数，表示这个参数对当前损失“负有多大责任”。如果某个参数的梯度分量很大，说明这个参数对当前损失的值有很大影响；如果某个参数的梯度分量很小，说明这个参数对当前损失的值影响不大。\n那么，如果梯度为 0 呢？我们是不是到达了一个最优点？\n不一定。梯度为 0 可能是一个局部最小值，也可能是一个局部最大值，还可能是一个鞍点（saddle point）。所以，虽然梯度为 0 是一个重要的条件，但并不代表我们一定找到了全局最小值。\n总结来说，梯度提供了两个关键信息：每个参数对当前损失的影响程度，以及在当前位置，损失函数变化最快的方向。梯度把一个整体的误差，拆解成每个参数的局部影响。但是，到目前为止，我们还没有回答一个关键的问题：我们手里只有损失函数的最终结果，那么如何高效地计算出每一个参数对应的梯度？为了解决这个问题，我们需要一种能够记录完整计算过程的结构，使梯度可以被系统地计算出来。这种结构就是计算图。",
    "crumbs": [
      "中文",
      "深度学习简介",
      "Chapter 1.3 前向传播、反向传播与计算图"
    ]
  },
  {
    "objectID": "zh/ch1-introduction/ch1.3-computation-graph.html#计算图",
    "href": "zh/ch1-introduction/ch1.3-computation-graph.html#计算图",
    "title": "Chapter 1.3 前向传播、反向传播与计算图",
    "section": "1.3.2 计算图",
    "text": "1.3.2 计算图\n在上一节中，我们介绍了梯度。梯度告诉我们，每个参数对损失函数的影响程度，也就是每个参数“负有多大责任”。但是，这里有一个关键问题：神经网络可能包含成千上万个参数，而损失函数往往是一个由大量加法、乘法、非线性函数层层嵌套形成的复杂函数。我们该如何高效地计算每一个参数对应的梯度？如果直接从最终的损失函数出发，对每一个参数分别求偏导数，不仅过程繁琐，而且计算效率极低。\n为了解决这个问题，我们需要一种方法，能够满足以下几个要求：\n\n知道损失函数是如何被一步一步计算出来的；\n每一个中间结果是由哪些变量产生的；\n每一个参数是如何影响最终损失的。\n\n这就是 计算图（Computational Graph）。\n计算图就是一个很好的“追责机制”。它告诉我们，损失函数的值是由哪些参数计算出来的，以及这些参数又依赖于哪些参数，以此类推。所以，计算图就好像一个“责任链条”一样，让我们知道每一个参数和输入在损失函数的值中扮演了什么角色。前向传播是沿着这个责任链条，逐步计算每个变量的数值；反向传播则是沿着责任链条的反方向，逐步计算每个变量对损失函数的影响程度，也就是梯度。\n现在我们来正式介绍一下计算图。计算图是一个有向无环图（Directed Acyclic Graph, DAG），它由 节点（Nodes） 和 边（Edges） 组成。“有向”表示数据流动有明确方向，“无环”表示不存在循环依赖。也就是说，一个变量不能依赖于它自己的计算结果。每个节点表示一个操作（Operation）或一个变量（Variable），每条边表示数据的流动方向。\n假设我们有一个很简单的函数：\n\\[ f(x,y) = \\sin(x \\cdot y) \\]\n我们可以把这个函数拆解成几个简单的操作：\n\n乘法操作：计算 \\(z = x \\cdot y\\)\n正弦操作：计算 \\(f = \\sin(z)\\)\n\n我们可以把这个过程表示成一个计算图：\n\n\n\n\n\nflowchart LR\n  x((x)) --&gt; mul[mul]\n  y((y)) --&gt; mul\n  mul --&gt; z((z))\n  z --&gt; sin[sin]\n  sin --&gt; f((f))\n\n\n\n\n\n\n在这个计算图中，圆圈表示变量，方框表示操作，箭头表示数据流动的方向。输入 \\(x\\) 和 \\(y\\) 通过乘法操作得到中间变量 \\(z\\)，然后 \\(z\\) 通过正弦操作得到输出 \\(f\\)。\n根据变量在图中的位置，我们通常把节点分为三类：\n\n叶子节点（Leaf Nodes）：没有输入的节点，例如 \\(x\\) 和 \\(y\\)。\n中间节点（Intermediate Nodes）：由其他变量计算得到的节点，例如 \\(z\\)。\n根节点（Root Node）：最终结果节点，例如 \\(f\\)。\n\n那么，为什么计算图必须是无环的呢？因为如果存在一个节点依赖于它自己的输出，就会形成一个循环。这样的循环会导致计算无法进行，因为我们无法确定哪个节点先计算，哪个节点后计算。\n搭建计算图的方式主要分为两种，一种是静态计算图，另一种是动态计算图。在程序运行前就已经完全定义好，之后就无法进行修改。而动态计算图则是在运行时动态构建的，可以根据需要进行修改。不同的深度学习框架可能采用不同的计算图方式，比如 TensorFlow 1.x 使用静态计算图，而 PyTorch 使用动态计算图。静态计算图的优点是优化空间大，执行效率高，而动态计算图的优点是可以根据不同输入灵活改变结构，更直观，而且易于调试。现代深度学习框架大多采用动态计算图。\n计算图最大的作用，就是把一个复杂的函数拆解成一系列简单操作，并明确记录它们之间的依赖关系。沿着这条依赖链，我们就可以系统地计算梯度。这就是反向传播的基础。在动态计算图框架中，搭建这个计算图本身，靠的就是前向传播。",
    "crumbs": [
      "中文",
      "深度学习简介",
      "Chapter 1.3 前向传播、反向传播与计算图"
    ]
  },
  {
    "objectID": "zh/ch1-introduction/ch1.3-computation-graph.html#前向传播与反向传播",
    "href": "zh/ch1-introduction/ch1.3-computation-graph.html#前向传播与反向传播",
    "title": "Chapter 1.3 前向传播、反向传播与计算图",
    "section": "1.3.3 前向传播与反向传播",
    "text": "1.3.3 前向传播与反向传播\n有了计算图这个“依赖链条”，我们就可以清楚地描述两个核心过程：前向传播和反向传播。简单来说，前向传播就是计算每个节点的数值，而反向传播就是计算每个节点的梯度。\n前向传播（Forward Propagation） 是指从输入开始，沿着计算图的方向，逐步计算每个中间节点的数值，直到得到最终输出的过程。同时，计算图会保存必要的中间变量，以便在反向传播时计算对应的梯度。\n例如，对于函数：\n\\[ f(x,y) = \\sin(x \\cdot y) \\]\n前向传播的计算顺序为：\n\n计算 \\(z = x \\cdot y\\)\n计算 \\(f = \\sin(z)\\)\n\n也就是说，每一个节点的值，都是由它的输入节点计算得到的。因此，前向传播完成了两件事：\n\n在计算图上计算每个节点的数值，并最终得到损失函数的值；\n同时记录计算过程（在动态计算图框架中），为反向传播提供必要的信息。\n\n反向传播（Backward Propagation） 是指从最终输出开始，沿着计算图的反方向，逐步计算每个变量对损失函数的梯度。反向传播的核心是 链式法则（Chain Rule）。一个变量对最终结果的影响，可以分解为多个局部影响的乘积。\n假设损失函数是 \\(L = L(f)\\)，我们想要计算输入变量 \\(x\\) 和 \\(y\\) 对损失函数的梯度 \\(\\frac{\\partial L}{\\partial x}\\) 和 \\(\\frac{\\partial L}{\\partial y}\\)。根据链式法则，我们可以沿着计算图的反方向，逐步计算出每个节点的梯度：\n\\[\n\\begin{aligned}\n\\frac{\\partial L}{\\partial x} &= \\frac{\\partial L}{\\partial f} \\cdot \\frac{\\partial f}{\\partial z} \\cdot \\frac{\\partial z}{\\partial x} = \\frac{\\partial L}{\\partial f} \\cdot \\cos(z) \\cdot y \\\\\n\\frac{\\partial L}{\\partial y} &= \\frac{\\partial L}{\\partial f} \\cdot \\frac{\\partial f}{\\partial z} \\cdot \\frac{\\partial z}{\\partial y} = \\frac{\\partial L}{\\partial f} \\cdot \\cos(z) \\cdot x\n\\end{aligned}\n\\]\n这里 \\(\\frac{\\partial L}{\\partial f}\\) 是损失函数对输出 \\(f\\) 的梯度，它由具体的损失函数形式决定。\n在反向传播中，我们从输出节点开始，逐步计算每个节点的梯度，并沿着计算图向输入方向传播。每个节点的梯度，都由两个因素决定：\n\n这个节点对最终输出的影响程度（局部梯度）；\n上游节点传递下来的梯度（链式法则中的乘积）。\n\n这种“局部梯度 × 上游梯度”的递推过程，使得我们能够高效地计算出所有参数的梯度，而无需重复计算整个函数。\n通过前向传播，我们搭建了计算图，计算了每个节点的数值；通过反向传播，我们沿着计算图的反方向，计算了每个节点的梯度。这样，我们就可以知道每个参数对损失函数的影响程度，从而指导我们如何调整参数来最小化损失函数。这就是深度学习的核心学习机制。",
    "crumbs": [
      "中文",
      "深度学习简介",
      "Chapter 1.3 前向传播、反向传播与计算图"
    ]
  },
  {
    "objectID": "zh/ch1-introduction/ch1.3-computation-graph.html#本章小结",
    "href": "zh/ch1-introduction/ch1.3-computation-graph.html#本章小结",
    "title": "Chapter 1.3 前向传播、反向传播与计算图",
    "section": "1.3.4 本章小结",
    "text": "1.3.4 本章小结\n在本章中，我们介绍了深度学习中最核心的三个概念：\n\n梯度：描述每个参数对损失函数的影响程度；\n计算图：记录函数的完整计算结构；\n前向传播与反向传播：分别用于计算数值和计算梯度。\n\n前向传播沿着计算图的方向计算损失函数，而反向传播沿着计算图的反方向计算梯度。正是由于计算图提供了清晰的依赖结构，我们才能高效地计算出神经网络中所有参数的梯度。\n在下一章中，我们将看到，如何利用这些梯度，逐步调整参数，使神经网络学会完成具体任务。",
    "crumbs": [
      "中文",
      "深度学习简介",
      "Chapter 1.3 前向传播、反向传播与计算图"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Redirect",
    "section": "",
    "text": "Choose language:\n\n中文\nEnglish\n\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "en/index.html",
    "href": "en/index.html",
    "title": "Deep Learning Notes",
    "section": "",
    "text": "Deep Learning Notes\nWelcome to my deep learning notes.\n\n\n\n\nReuseCC BY-NC 4.0"
  },
  {
    "objectID": "zh/ch1-introduction/ch1.1-basic-concept.html",
    "href": "zh/ch1-introduction/ch1.1-basic-concept.html",
    "title": "Chapter 1：深度学习中的基本概念",
    "section": "",
    "text": "相信很多人都听说过深度学习，知道深度学习是一个黑盒子。我们给神经网络一些数据，神经网络给我们输出一些数据。但是，直到今天，我们仍然不知道神经网络内部到底发生了什么，为什么神经网络就这么有用。"
  },
  {
    "objectID": "zh/ch1-introduction/ch1.1-basic-concept.html#什么是张量",
    "href": "zh/ch1-introduction/ch1.1-basic-concept.html#什么是张量",
    "title": "Chapter 1：深度学习中的基本概念",
    "section": "1.1.1 什么是张量？",
    "text": "1.1.1 什么是张量？\n相信大家都熟悉 NumPy，知道 NumPy 中的 NdArray 是一个多维数组，可以用来存储和操作数据。在深度学习中，我们也有一个类似的概念，叫做 张量（tensor）。张量是深度学习中的基本数据结构，0 维的张量是标量，1 维的张量是向量，2 维的张量是矩阵，更高维的张量则对应多维数组。然而，与 NumPy 不同的是，在深度学习中，张量不仅仅是一个高维数组，它的每一个维度都携带着明确的语义。\n以最常见的二维张量为例，它的形状通常写作 (B, F)。第一个维度 B 对应 Batch Size（批量大小），表示一次并行处理的样本数量。你可以把它理解为神经网络一次性处理 B 个样本，以充分发挥并行计算的优势。而第二个维度 F 对应 Features（特征维度），表示每个样本所具有的特征。这种“行是样本，列是特征”的约定，和 Excel 表格一样，不仅便于我们理解，也使得矩阵运算可以自然地表达对一批数据的统一变换。\n当数据具有空间或时序结构时，张量的维度会进一步增加。在卷积神经网络中，形状为 (B, C, H, W) 的四维张量通常用于表示一批图像，其中 C 表示通道数（例如 RGB 图像的通道数为 3），H 和 W 分别对应空间上的高度和宽度；在循环神经网络中，形状 (B, T, D) 的三维张量则往往表示批量大小、时间步以及每个时间步上的特征维度。当然，也有五维或更高维的张量，例如在视频处理或多模态学习中，形状可能是 (B, C, T, H, W)，其中 T 表示时间维度，H 和 W 分别表示空间上的高度和宽度。\n深度学习模型往往假设输入的张量符合预先设定好的结构。如果一不小心把维度搞错了，或者把维度的顺序搞错了，运气好模型会报错，运气不好模型就会默默地学出一个完全错误的函数映射。正因为如此，理解张量的结构和语义，是设计和调试深度学习模型的第一步。"
  },
  {
    "objectID": "zh/ch1-introduction/ch1.1-basic-concept.html#什么是神经网络",
    "href": "zh/ch1-introduction/ch1.1-basic-concept.html#什么是神经网络",
    "title": "Chapter 1：深度学习中的基本概念",
    "section": "1.1.2 什么是神经网络？",
    "text": "1.1.2 什么是神经网络？\n了解了什么是张量，我们来聊聊神经网络本身。神经网络是由一系列层（Layer）组成的，每一层都包含一些参数（Parameter），这些参数是我们在训练过程中需要学习的。每一层都会对输入的数据进行某种变换，最终输出一个新的张量。通过堆叠不同的层，我们就可以构建出一个复杂的函数映射，从而实现各种各样的任务，比如图像分类、自然语言处理等等。\n简单来说，神经网络就是一个复杂的函数 \\(y = f(x)\\)。和传统机器学习不同，这个 \\(f\\) 是神经网络自己学习出来的，而不是我们手动提取特征设计的。输入 \\(x\\) 可以是图像、音频、文本，输出 \\(y\\) 可以是分类标签、预测的数值、生成的文本。神经网络通过调整内部的参数，使得这个函数 \\(f\\) 能够尽可能准确地把输入映射到输出。我们的任务，就是给神经网络提供数据，让神经网络自己学习这个复杂的函数 \\(f\\)。\n其实，搭建神经网络就是在搭积木，每一层都是一个积木块，我们把它们按照一定的顺序堆叠起来，就得到了一个完整的神经网络。不同的层有不同的功能，比如全连接层（Fully Connected Layer）可以实现线性变换，卷积层（Convolutional Layer）可以提取图像的局部和全局特征，循环层（Recurrent Layer）可以处理时间序列和文本数据，等等。通过合理地选择和组合这些层，我们就可以设计出适合我们任务的神经网络。当然，搭建的好与不好，还需要通过训练来验证。训练的过程就是不断调整神经网络中的参数，使得它在训练数据上的表现越来越好，从而达到我们预期的目标。"
  },
  {
    "objectID": "zh/ch10-flash-attention/ch10.1-attention-is-io-bound.html",
    "href": "zh/ch10-flash-attention/ch10.1-attention-is-io-bound.html",
    "title": "Chapter 10.1 为什么 Attention 是 IO-Bound",
    "section": "",
    "text": "在前面的章节里，我们已经从数学结构上理解了 Attention：给定 \\(Q, K, V\\)，我们先计算相似度矩阵 \\(S = QK^\\top\\)，再做 softmax，最后得到输出 \\(O = \\text{softmax}(S)V\\)。从公式上看，它不过是几次矩阵乘法和一个逐元素的非线性操作。按直觉来说，矩阵乘法是 GPU 最擅长的事情，因此 Attention 理应是一个计算密集型的算子。\n但当我们真正去分析它在 GPU 上的执行过程时，就会发现，Attention 往往不是被算力限制，而是被内存访问限制。也就是说，它更像是一个 IO-bound 问题，而不是 compute-bound 问题。这意味着，即使我们的 GPU 还有大量浮点算力没有被用满，Attention 也可能已经跑不动了，因为瓶颈不在算，而在搬。那么，问题出在哪儿了呢？\n答案就藏在 Attention 的注意力矩阵里。这个矩阵的规模是 \\(n \\times n\\) 的，随序列长度平方增长。当序列长度变大时，真正拖慢速度的，往往不是乘法算子本身，而是频繁地在 GPU 的 HBM（高带宽显存）和 SRAM（片上缓存）之间来回读写大块数据。也就是说，Attention 的性能瓶颈，本质上来自数据移动，而不是算术运算。\n接下来，我们就从一个更底层的视角出发，拆解 Attention 在硬件上的真实执行过程，看看它到底把时间花在了哪里。",
    "crumbs": [
      "中文",
      "FlashAttention：高效的注意力机制实现",
      "Chapter 10.1 为什么 Attention 是 IO-Bound"
    ]
  },
  {
    "objectID": "zh/ch10-flash-attention/ch10.1-attention-is-io-bound.html#gpu-性能模型与算术强度",
    "href": "zh/ch10-flash-attention/ch10.1-attention-is-io-bound.html#gpu-性能模型与算术强度",
    "title": "Chapter 10.1 为什么 Attention 是 IO-Bound",
    "section": "10.1.1 GPU 性能模型与算术强度",
    "text": "10.1.1 GPU 性能模型与算术强度\n在介绍 Attention 的时候，我们提出了一个看起来有些反直觉的结论：Attention 的瓶颈往往不在算力，而在内存访问。但是，在分析 Attention 为什么是 IO-bound 之前，我们需要先建立一个最基本的 GPU 性能视角。\n当我们说一个算子“快”或者“慢”时，它到底受什么限制？和《计算机组成原理》里讲的一样，只有两类资源会成为瓶颈：\n\n算术计算能力（Compute Throughput）：每秒能做多少次浮点运算（FLOPs）\n内存带宽（Memory Bandwidth）：每秒能从内存搬运多少字节数据（Bytes/s）\n\n任何一个算子，在执行时都要做两件事：读取数据，对数据进行计算。如果算力先被耗尽，我们称它是 compute-bound 的；如果内存带宽先被耗尽，我们称它是 IO-bound 的。那么，我们如何判断一个算子更可能受哪种资源限制？\n在高性能计算领域，有一个非常核心的指标，那就是 算数强度（Arithmetic Intensity）：\n\\[ \\text{Arithmetic Intensity} = \\frac{\\text{FLOPs}}{\\text{Bytes Moved}} \\]\n也就是说，每搬运 1 字节数据，能够做多少次浮点运算。这个指标直接决定了算子更接近 compute-bound 还是 IO-bound。\n从直觉上可以这样理解：\n\n如果一个算子读一点数据，就反复计算很多次，那么它的算术强度高，就更可能是 compute-bound；\n如果一个算子读一点数据，只算几下就丢掉了，那么它的算术强度低，就更可能是 IO-bound。\n\n举个简单的例子，矩阵乘法为什么通常是 compute-bound？\n因为在矩阵乘法中，每个输入元素都会被复用多次。例如，在 \\(C = AB\\) 中，矩阵 \\(A\\) 的每个元素会被用来计算 \\(C\\) 中的一整行，而矩阵 \\(B\\) 的每个元素会被用来计算 \\(C\\) 中的一整列。假设矩阵 \\(A\\) 和 \\(B\\) 的大小都是 \\(n \\times n\\)，那么每个元素被复用的次数大约是 \\(n\\) 次。因此，矩阵乘法的算术强度大约是 \\(O(n)\\)，也就通常是 compute-bound。\n同时，我们还要考虑一个点，那就是 GPU 的算力增长速度远远快于显存带宽的增长速度。这种现象被称为 内存墙（Memory Wall）。也就是说，现代 GPU 的 FLOPs 数字非常惊人，但显存带宽的提升是相对缓慢的。如果一个算子不能有效地复用数据，而是频繁地从 HBM 里读写大块张量，那么即使算术本身不复杂，也会被内存带宽卡住。\n而且，和 CPU 一样，GPU 的内存并不是一个统一空间，而是分层的：\n\n\n\nGPU 内存分层模型\n\n\n\nHBM（High Bandwidth Memory）：容量大，但访问延迟高，速度约为 1.5 TB/s\nSRAM / Shared Memory / L2 Cache：容量小，但速度快，访问延迟低，速度约为 19 TB/s\n寄存器（Register）：容量极小，但最快，访问延迟几乎为零\n\n所以，一个算子真正高效的方式，就尽量把数据留在片上（SRAM 或者寄存器），减少对 HBM 的访问。因为一旦频繁访问 HBM，即使每次访问都“很快”，累积起来仍然会成为性能瓶颈。\n那么，Attention 里的 \\(QK^\\top\\)、softmax 和 \\(V\\)，真的像矩阵乘法那样有良好的数据复用吗？还是说，它在结构上就天然缺乏足够高的算术强度？",
    "crumbs": [
      "中文",
      "FlashAttention：高效的注意力机制实现",
      "Chapter 10.1 为什么 Attention 是 IO-Bound"
    ]
  },
  {
    "objectID": "zh/ch10-flash-attention/ch10.1-attention-is-io-bound.html#标准-attention-的-io-分析",
    "href": "zh/ch10-flash-attention/ch10.1-attention-is-io-bound.html#标准-attention-的-io-分析",
    "title": "Chapter 10.1 为什么 Attention 是 IO-Bound",
    "section": "10.1.2 标准 Attention 的 IO 分析",
    "text": "10.1.2 标准 Attention 的 IO 分析\n现在，我们已经有了一些判断工具：\n\n看 FLOPs\n看数据搬运量\n判断算子的算术强度\n判断 compute-bound 还是 IO-bound\n\n这一节，我们就把它真正用在标准 Attention 上。\n我们从最常见的形式出发（忽略 batch 和 head 维度，只看单头，忽略缩放）：\n\\[ S = QK^\\top, \\quad P = \\text{softmax}(S), \\quad O = PV \\]\n设序列长度为 \\(n\\)，特征维度为 \\(d\\)，那么：\n\n\\(Q, K, V \\in \\mathbb{R}^{n \\times d}\\)\n\\(S, P \\in \\mathbb{R}^{n \\times n}\\)\n\\(O \\in \\mathbb{R}^{n \\times d}\\)\n\n我们来分别分析 FLOPs 和 IO。\n\n10.1.2.1 FLOPs 分析\n\n第一步：计算 \\(QK^\\top\\)。这是一个矩阵乘法，涉及 \\(n \\times d\\) 和 \\(d \\times n\\) 的乘积，结果是 \\(n \\times n\\) 的矩阵。FLOPs 大约是 \\(2n^2d\\)（每个输出元素需要 \\(d\\) 次乘加操作，总共有 \\(n^2\\) 个输出元素，一次乘加算 2 次 FLOPs）。\n第二步，计算 softmax。softmax 的 FLOPs 主要来自于指数运算和归一化。每个元素需要计算一个指数（1 FLOP）和一个除法（1 FLOP），FLOPs 大约是 \\(2n^2\\)（忽略处理数值稳定性的额外 FLOPs）。\n第三步，计算 \\(PV\\)。这是另一个矩阵乘法，涉及 \\(n \\times n\\) 和 \\(n \\times d\\) 的乘积，结果是 \\(n \\times d\\) 的矩阵。FLOPs 大约是 \\(2n^2d\\)。\n\n把这些加起来，总的 FLOPs 大约是：\n\\[ \\text{FLOPs} \\approx 2n^2d + 2n^2 + 2n^2d = 4n^2d + 2n^2 \\approx O(n^2d) \\]\n\n\n10.1.2.2 IO 分析\n我们现在关心的是：需要从 HBM 搬多少数据？\n先看标准 Attention 的计算流程：\n\n从 HBM 读取 \\(Q, K\\)，计算 \\(S\\)\n把 \\(S\\) 写回 HBM\n从 HBM 读取 \\(S\\)，计算 \\(P\\)\n把 \\(P\\) 写回 HBM\n从 HBM 读取 \\(P, V\\)，计算 \\(O\\)\n把 \\(O\\) 写回 HBM\n\n从流程里不难发现，中间的 \\(n \\times n\\) 矩阵被完整写入、再读回，造成了大量的数据搬运。注意，因为这里的矩阵都很大，SRAM 装不下，所以只能把它们放在 HBM 里，每次读写都要经过内存总线，造成了巨大的 IO 开销。\n我们来具体计算一下。\n\n输入读取。我们需要从 HBM 里读取 \\(Q, K, V\\)，每个都是 \\(n \\times d\\) 的矩阵，所以总共是 \\(3nd\\) 个元素，每个元素 4 字节（假设元素类型是 float32），总共是 \\(8nd\\) 字节。\n中间矩阵 \\(S\\) 的读写。\\(S\\) 是 \\(n \\times n\\) 的矩阵，每个元素 4 字节，所以读写一次就是 \\(8n^2\\) 字节。\n中间矩阵 \\(P\\) 的读写。\\(P\\) 也是 \\(n \\times n\\) 的矩阵，每个元素 4 字节，所以读写一次也是 \\(8n^2\\) 字节。\n输出写回。输出 \\(O\\) 是 \\(n \\times d\\) 的矩阵，每个元素 4 字节，所以写回是 \\(8nd\\) 字节。\n\n把这些加起来，总的 IO 大约是：\n\\[ \\text{IO} \\approx 8nd + 8n^2 + 8n^2 + 8nd = 16n^2 + 16nd \\approx O(n^2) \\]\n\n\n10.1.2.3 算术强度分析\n现在我们有了 FLOPs 和 IO 的估计，我们就可以计算算术强度：\n\\[ \\text{Arithmetic Intensity} = \\frac{\\text{FLOPs}}{\\text{Bytes Moved}} \\approx \\frac{4n^2d}{16n^2} = O(d) \\]\n计算得到 Attention 的算术强度是 \\(O(d)\\)。这是一个常数，在实际应用中，\\(d\\) 通常为 64、128 或 256，因此算术强度通常在 16~64 FLOPs/byte 的范围内。相比之下，现代 GPU（例如 NVIDIA A100）的峰值计算能力与内存带宽之比约为 200 FLOPs/byte。这意味着，只有当算术强度显著高于这一阈值时，计算才能达到 compute-bound 状态，而 Attention 的算术强度远低于该阈值。因此，其性能受到内存带宽的限制，而不是计算能力的限制。\n所以，Attention 属于典型的 memory-bound，其性能瓶颈在于 HBM 与片上存储之间的数据搬运，而不是浮点运算本身。",
    "crumbs": [
      "中文",
      "FlashAttention：高效的注意力机制实现",
      "Chapter 10.1 为什么 Attention 是 IO-Bound"
    ]
  },
  {
    "objectID": "zh/ch10-flash-attention/ch10.1-attention-is-io-bound.html#为什么长序列会放大-io-问题",
    "href": "zh/ch10-flash-attention/ch10.1-attention-is-io-bound.html#为什么长序列会放大-io-问题",
    "title": "Chapter 10.1 为什么 Attention 是 IO-Bound",
    "section": "10.1.3 为什么长序列会放大 IO 问题",
    "text": "10.1.3 为什么长序列会放大 IO 问题\n在上一节里，我们已经看到，标准 Attention 的 IO 主导项是那个 \\(n \\times n\\) 的中间矩阵 \\(S\\)。现在我们进一步问一个问题：如果序列长度 \\(n\\) 变大，问题会如何演化？\n我们已经知道，\\(\\text{FLOPs} \\approx O(n^2d)\\)，而 \\(\\text{IO} \\approx O(n^2)\\)。也就是说，当序列长度翻倍时，IO 增长 4 倍。而 GPU 的带宽是固定的。这不是一个慢慢增长的问题，而是一个爆炸式增长的问题。\n长序列带来的问题有两个。一个是注意力矩阵 \\(S\\) 的大小，当 \\(n=2048\\) 时，\\(S\\) 的大小已经是 16MB（假设 float32），而当 \\(n=4096\\) 时，\\(S\\) 的大小就达到了 64MB，而且这还只是单头。还要加上注意力头数量，批处理大小，显存压力会迅速变得不可接受。另外一个问题是带宽。容量问题还可以通过分布式计算等方式缓解，但带宽问题无法绕开。每次访问 \\(S\\) 都需要从 HBM 里搬运大量数据，这会成为性能的主要瓶颈，导致 Attention 在长序列上的效率急剧下降。\n因此，长序列不仅增加了计算量，更重要的是，它极大地放大了 IO 问题，使得标准 Attention 在处理长序列时变得非常低效。而且，由于内存墙的存在，GPU 算力的提升速度要远远快于内存带宽的提升速度。对于 compute-bound 的算子来说，算力的提升可以直接转化为性能提升；但对于 IO-bound 的算子来说，算力的提升并不能带来性能提升，因为瓶颈在内存访问上。因此，随着序列长度的增加，Attention 的性能问题会变得越来越严重。\n如果我们真正理解了这一点，那么一个自然的问题就会出现：我们是否真的需要把整个 \\(S\\) 存下来？\n如果答案是“不需要”，那么我们就可以把 IO 的复杂度从 \\(O(n^2)\\) 降到接近 \\(O(nd)\\)，从而把 Attention 推向 compute-bound，使其不再受内存带宽的限制。这就是 FlashAttention 想做的事情。\n下一节，我们将正式进入 FlashAttention 的核心思想，利用矩阵分块、在线更新和 kernel 融合来消灭 IO。",
    "crumbs": [
      "中文",
      "FlashAttention：高效的注意力机制实现",
      "Chapter 10.1 为什么 Attention 是 IO-Bound"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.2-autograd-contexts.html",
    "href": "zh/ch2-pytorch-introduction/ch2.2-autograd-contexts.html",
    "title": "Chapter 2.2 PyTorch 中的梯度记录与控制",
    "section": "",
    "text": "在 2.1 节里，我们回答了一个问题：梯度是怎么被算出来的？前向传播时记账，反向传播时查账，Autograd 把计算图搭起来，再沿着图把梯度传回去。\n但是，当我们写代码时，很快会遇到另一个更现实的问题：这本账，到底要不要记？\n训练时当然要记，因为我们需要反向传播。可是在验证、推理、只做特征提取、或者只是想跑一遍模型看看输出的时候，记账反而是在浪费。它会保存中间结果、构建计算图、占显存，还可能让你不小心把一段本来只想算数值的代码拖进反向传播里。\n所以这一节我们换个视角：不再讨论怎么求导，而是讨论哪些计算会被 Autograd 记录，哪些会被忽略。PyTorch 给了我们几种很直接的开关：torch.no_grad()、torch.enable_grad()、torch.set_grad_enabled()，以及更偏推理优化的 torch.inference_mode()。它们不会改变你算出来的数值结果，但会改变这段计算有没有计算图、能不能反传、以及会花多少内存和开销。\n这也代表了 PyTorch 的一个设计理念：计算怎么做是算子的事，要不要记账是 Autograd 的事。接下来我们就从最常见的 no_grad() 开始，了解这些梯度记录模式。\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.2 PyTorch 中的梯度记录与控制"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.2-autograd-contexts.html#torch.no_grad暂停记账",
    "href": "zh/ch2-pytorch-introduction/ch2.2-autograd-contexts.html#torch.no_grad暂停记账",
    "title": "Chapter 2.2 PyTorch 中的梯度记录与控制",
    "section": "2.2.1 torch.no_grad()：暂停记账",
    "text": "2.2.1 torch.no_grad()：暂停记账\n在默认情况下，只要张量的 requires_grad=True，并且我们对它进行了运算，PyTorch 就会自动构建计算图。也就是说，只要你在可求导的范围里做计算，Autograd 就会默默地帮你把账记下来。但是有时候，我们并不需要这本账。\n比如，在验证模型性能时，我们通常不需要计算梯度，因为我们不会进行反向传播。或者，在推理阶段，我们只关心模型的输出结果，而不关心它是怎么算出来的。这时候，如果我们继续让 Autograd 记账，不仅浪费内存，还可能导致性能下降。如果还让 Autograd 构建计算图，那就是多此一举。\n因此，PyTorch 提供了 torch.no_grad() 上下文管理器（也可以当函数的装饰器），让我们可以明确告诉 Autograd：在这个代码块里，我们不需要它记账。\n我们来看一个最直观的对比。在默认模式下：\n\nmodel = nn.Linear(6, 4)\nx = torch.randn(10, 6)\ny = torch.randn(10, 4)\n\ny_pred = model(x)\nprint('y_pred.requires_grad before no_grad():', y_pred.requires_grad)\n\ny_pred.requires_grad before no_grad(): True\n\n\n输出会是 True，因为模型参数默认 requires_grad=True，所以结果自动进入计算图。\n现在我们把这段前向传播放进 no_grad() 里：\n\nwith torch.no_grad():\n    y_pred = model(x)\n\nprint('y_pred.requires_grad inside no_grad():', y_pred.requires_grad)\n\ny_pred.requires_grad inside no_grad(): False\n\n\n这一次的输出是 False。\n注意，在 no_grad() 模式里，所有前向传播正常执行，只是得到的结果不再被 Autograd 追踪。而一旦某个张量不再被追踪，后续所有基于它的计算也都不再被追踪。如果我们此时对一个不再被追踪的张量调用 backward()，就会报错，因为这个张量压根不在计算图里，自然也就没法执行反向传播。\n\nloss = F.mse_loss(y_pred, y)\ntry:\n    loss.backward()\nexcept RuntimeError as err:\n    print('RuntimeError:', err)\n\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n\n\n这里的 loss 虽然不在 no_grad() 里，但它是基于 y_pred 计算出来的，而 y_pred 已经不被追踪了，并且另外一个输入 y 也并没有请求梯度。所以，loss 也就被排除在整个计算图里。如果我们对这个 loss 调用 backward()，就会报错。\n可能有的人会以为，no_grad() 会把某些张量的 requires_grad 属性改成 False，但其实并不是。no_grad() 只是告诉 Autograd 不要追踪这个代码块里的计算，但它并不会修改张量本身的属性。你可以在 no_grad() 外创建一个新的张量，它的 requires_grad 仍然是 True，但是这并不妨碍它在 no_grad() 里被当成普通张量来使用，不会被追踪。\n\nx = torch.randn(10, 6, requires_grad=True)\n\nwith torch.no_grad():\n    print('x.requires_grad inside no_grad():', x.requires_grad)\n    y_pred = model(x)\n    print('y_pred.requires_grad inside no_grad():', y_pred.requires_grad)\n\nx.requires_grad inside no_grad(): True\ny_pred.requires_grad inside no_grad(): False\n\n\n所以，no_grad() 并没有阻止张量本身的“可导资格”，它只是阻止在这个上下文里产生的新计算被记录。也就是说，张量的 requires_grad 属性是一种“能力声明”，表示“我有资格被追踪”，而 no_grad() 是一种“行为控制”，表示“在这个上下文里，不要追踪任何计算”，这两者是相互独立的。\n此外，在 no_grad() 里创建的新张量，如果我们后续想让它重新加入自动微分系统，仍然可以通过调用 requires_grad_() 方法来实现。比如：\n\nwith torch.no_grad():\n    x = torch.randn(10, 6)\n    print('x.requires_grad inside no_grad():', x.requires_grad)\n\nx.requires_grad_()\nprint('x.requires_grad after requires_grad_():', x.requires_grad)\n\nx.requires_grad inside no_grad(): False\nx.requires_grad after requires_grad_(): True\n\n\n也就是说，no_grad() 是暂时关闭记录，而不是永久剥夺张量的“可导资格”。在内部，仍然维护了一系列计数器，来确保当我们后续需要重新开启梯度记录时，能够正确地恢复状态。但是，这仍然会引入一定的计算和显存开销。这一点和后面我们要讲的 inference_mode() 会形成一个非常重要的对比。在 inference_mode() 中，PyTorch 不仅不追踪，还会彻底关闭一些与 Autograd 相关的功能，使得我们无法再通过 requires_grad_() 来重新开启梯度记录。\n从一个更底层的角度理解 no_grad() 就会发现，在 PyTorch 里，计算是数值层面的行为，记录是自动微分系统的行为。而 no_grad() 只影响后者。这也是为什么我们常在模型验证、推理部署、参数更新里用到它。\n那么，接下来一个自然的问题是：如果梯度可以被关闭，那么它能不能在局部重新开启？如果我们在推理阶段的某个小步骤突然需要梯度怎么办？这就引出了下一节：torch.enable_grad()。",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.2 PyTorch 中的梯度记录与控制"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.2-autograd-contexts.html#torch.enable_grad重新开始记账",
    "href": "zh/ch2-pytorch-introduction/ch2.2-autograd-contexts.html#torch.enable_grad重新开始记账",
    "title": "Chapter 2.2 PyTorch 中的梯度记录与控制",
    "section": "2.2.2 torch.enable_grad()：重新开始记账",
    "text": "2.2.2 torch.enable_grad()：重新开始记账\n在上一节里，我们看到 no_grad() 可以让 Autograd 暂停记录。那么一个自然的问题来了：如果我们已经在 no_grad() 里了，能不能只对其中一小段计算重新开启梯度？\n答案是可以的。这就是 enable_grad() 的作用。\n当然，也可以在外层用 enable_grad() 来开启梯度，然后在内层用 no_grad() 来关闭，这些都是可以嵌套使用的。但是，在默认模式下开启 enable_grad() 等同于啥也没干，所以也就懒得写了。\n还是先来看一个简单的例子：\n\nx = torch.randn(10, 6, requires_grad=True)\n\nwith torch.no_grad():\n    y = x * 3  # Does not record computation graph\n    print('y.requires_grad in no_grad():', y.requires_grad)\n\n    with torch.enable_grad():\n        z = x * 4  # Enables gradient tracking\n        print('z.requires_grad in enable_grad():', z.requires_grad)\n\n# Only z will have gradients tracked\nz.backward(gradient=torch.ones_like(z))\n\ny.requires_grad in no_grad(): False\nz.requires_grad in enable_grad(): True\n\n\n这里发生的事情非常关键：外层 no_grad() 关闭了自动微分记录，内层 enable_grad() 又在局部恢复了记录。而在退出内层的 enable_grad() 之后，外层的 no_grad() 仍然有效，所以后续的计算又回到了不被追踪的状态。这说明梯度模式是栈式管理的。进入一个上下文，就压入一种模式；退出这个上下文，就恢复之前的模式。\n那么，这有什么意义呢？\n很多时候，我们的代码路径是共用的。比如，推理阶段的大部分前向都不需要梯度，但某个中间步骤需要做敏感性分析；或者某些调试代码希望临时计算一个梯度。如果没有 enable_grad()，我们就不得不把整段代码拆开，或者频繁地在外层切换状态。但有了 enable_grad()，我们就可以在需要的地方局部开启，而不影响整体的推理流程。\n当然，还有一个更通用的接口，就是 torch.set_grad_enabled()，它可以接受一个布尔值参数，来直接设置当前的梯度模式。no_grad() 和 enable_grad() 其实就是这个接口的一个特例。\n\nx = torch.randn(10, 6)\nis_training = False\n\nwith torch.set_grad_enabled(is_training):\n    y_pred = model(x)\n\n当 is_training=True 时，等价于 enable_grad()；当 is_training=False 时，等价于 no_grad()。这使得代码逻辑更加统一，写条件控制也更方便。\n到目前为止，我们已经介绍了两种常用的梯度控制上下文：no_grad() 和 enable_grad()。它们分别用于关闭和开启梯度记录，并且可以嵌套使用，形成一个灵活的栈式管理系统。接下来，我们还会介绍一个更专门针对推理优化的上下文：torch.inference_mode()，它在性能和内存效率上比 no_grad() 更进一步。",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.2 PyTorch 中的梯度记录与控制"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.2-autograd-contexts.html#torch.inference_mode干脆以后都别记账了",
    "href": "zh/ch2-pytorch-introduction/ch2.2-autograd-contexts.html#torch.inference_mode干脆以后都别记账了",
    "title": "Chapter 2.2 PyTorch 中的梯度记录与控制",
    "section": "2.2.3 torch.inference_mode()：干脆以后都别记账了",
    "text": "2.2.3 torch.inference_mode()：干脆以后都别记账了\n在前面两节中，我们已经有了一个相当灵活的机制：\n\nno_grad() 可以关闭梯度记录；\nenable_grad() 可以局部恢复梯度记录；\nset_grad_enabled() 是一个更通用的接口，可以直接设置当前的梯度模式；\n梯度模式是可嵌套、可恢复的。\n\n从表面上看，似乎已经足够了。那为什么 PyTorch 还要再提供一个 inference_mode()？\n答案在于一个更深层的问题：如果我们不仅知道“当前不需要梯度”，还知道“这段计算以后永远不可能参与反向传播”，那么框架是不是可以做得更激进一点？把所有和梯度有关的开销全部去掉？\n这就是 inference_mode() 的设计动机。\n在 no_grad() 模式里，PyTorch 还会维护版本计数器（version counter）和视图追踪（view tracking），以及一些用于确保梯度正确性的内部检查。这些机制在训练时是必要的。它们可以防止原地操作破坏图结构，或者共享内存导致梯度错误。但在纯推理阶段，它们其实是额外开销。既然这一段代码的结果永远不会参与梯度计算，框架就可以不再维护与梯度相关的版本检查与视图追踪，做更激进的内存优化。因此，inference_mode() 通常会比 no_grad() 更快、更省内存。\n但是，它是不可逆的。\n我们知道，在 no_grad() 里创建的张量，可以在之后重新启用梯度：\n\nwith torch.no_grad():\n    x = torch.randn(10, 6)\n\nx.requires_grad_()  # we can still enable gradients for x\nprint('x.requires_grad after enabling:', x.requires_grad)\n\nx.requires_grad after enabling: True\n\n\n但在 inference_mode() 中创建的张量，如果我们尝试设置 requires_grad=True，就会直接报错：\n\nwith torch.inference_mode():\n    x = torch.randn(10, 6)\n\ntry:\n    x.requires_grad_()\nexcept RuntimeError as err:\n    print('RuntimeError:', err)\n\nRuntimeError: Setting requires_grad=True on inference tensor outside InferenceMode is not allowed.\n\n\n因为 inference_mode() 不是暂时关闭记录，而是创建了一种特殊的推理张量（inference tensor）。这类张量被标记为“永远不会进入自动微分系统”。即使你之后开启梯度模式，它们也不会被纳入计算图。所以，no_grad() 是暂时关闭，而 inference_mode() 是永久关闭。如果我们能够确定这段代码永远只用于推理，那就可以使用 inference_mode()。",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.2 PyTorch 中的梯度记录与控制"
    ]
  },
  {
    "objectID": "zh/ch2-pytorch-introduction/ch2.2-autograd-contexts.html#不同梯度模式下的行为对比",
    "href": "zh/ch2-pytorch-introduction/ch2.2-autograd-contexts.html#不同梯度模式下的行为对比",
    "title": "Chapter 2.2 PyTorch 中的梯度记录与控制",
    "section": "2.2.4 不同梯度模式下的行为对比",
    "text": "2.2.4 不同梯度模式下的行为对比\n到这里，我们其实已经看到三种不同的梯度语义：默认模式、no_grad() 模式和 inference_mode() 模式。它们代表的是三种不同强度的语义承诺，也对应不同使用场景下的性能权衡。\n在默认模式下，Autograd 必须假设当前的任何计算，都可能参与反向传播。因此它会：\n\n构建完整计算图\n保存反向传播所需的中间结果\n维护版本计数器和视图一致性检查\n\n这种模式是默认模式，灵活，但代价最高。通常用于模型训练阶段的前向传播。\n当我们进入 no_grad() 时，我们表达的是一种阶段性声明：这一段计算当前不参与反向传播。\n于是，在这种模式下，Autograd 可以做出一些优化：\n\n不再构建计算图\n不再保存中间结果\n但仍然保留 Autograd 的内部一致性机制\n退出该上下文后可以恢复正常梯度模式\n\n这种模式是暂时关闭。灵活性仍然存在，但性能已经有了明显提升。大多用于参数验证或模型评估阶段。\n而 inference_mode() 则是更强的承诺：这一段计算永远不会参与梯度计算。\n基于这个前提，Autograd 可以做出更激进的优化：\n\n不构建计算图\n跳过与梯度相关的版本检查与视图追踪\n在该模式下创建的张量无法再重新加入自动微分系统\n\n这是不可逆的关闭。这种模式的优化最激进，但限制也最大。适用于纯推理、模型评估和数据处理等场景。",
    "crumbs": [
      "中文",
      "PyTorch 入门",
      "Chapter 2.2 PyTorch 中的梯度记录与控制"
    ]
  }
]