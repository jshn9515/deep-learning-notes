{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9139deae",
   "metadata": {},
   "source": [
    "# Chapter 2.1 PyTorch 中的自动微分\n",
    "\n",
    "在 1.3 节里，我们把计算图当成一条“责任链”：损失函数为什么是这个值，沿着链条往回追，就能追到每个参数到底“负了多少责任”。这一节我们换一个更工程的视角：框架是怎么把这条责任链自动搭起来，并且在需要的时候把梯度算出来的？\n",
    "\n",
    "先把问题说得更直白一点：训练时我们要的是梯度，但我们手里只有一堆代码：加法、乘法、卷积、激活函数...。这些操作在前向传播里一行行执行，最后吐出一个 `loss`。那么，梯度从哪来？难道框架真的去推导一个巨大的符号表达式吗？\n",
    "\n",
    "当然不是。深度学习框架做的事情更像是：\n",
    "\n",
    "- 前向传播时顺手记账：你做了哪些操作？每一步依赖谁？中间结果是什么？\n",
    "- 反向传播时按账本回溯：从 `loss` 开始往回走，遇到一个操作就用它自己的“局部求导规则”，把梯度继续传下去。\n",
    "\n",
    "理解这套机制很关键。它不仅解释了“梯度是怎么来的”，还会直接影响我们后面遇到的许多现象：比如梯度为什么会累积？为什么中间变量默认没有 `.grad` 属性？为什么有些操作会切断梯度链条？以及显存与计算之间为什么总要做权衡。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "341d5119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd.functional as AF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbdc078",
   "metadata": {},
   "source": [
    "## 2.1.1 计算图不是画出来的，是跑出来的\n",
    "\n",
    "理解 PyTorch 的自动微分，最好的方式不是先背概念，而是先观察一件事：你只是在做前向计算，但计算图会在运行过程中自动搭建出来。\n",
    "\n",
    "假设我们有这样一个简单的函数：\n",
    "\n",
    "$$ z = \\sin(x \\cdot y) $$\n",
    "\n",
    "我们可以把它拆解成几个基本的运算步骤：\n",
    "\n",
    "1. 计算向量内积：$q = x \\cdot y$\n",
    "2. 计算正弦函数：$z = \\sin(q)$\n",
    "\n",
    "然后，我们告诉 PyTorch，在接下来的计算中，我们希望得到 `z` 关于 `x` 和 `y` 的梯度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8c612f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(1.0, 5.0, requires_grad=True)\n",
    "y = torch.arange(5.0, 9.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d072f1c",
   "metadata": {},
   "source": [
    "这里的 `requires_grad=True` 可以理解成一种声明：这些变量需要被“追责”。之后只要某个结果是由它们参与计算得到的，它就会自动带上可导属性，并在背后记录“我是谁算出来的，依赖了谁”。\n",
    "\n",
    "现在做两步普通的前向计算：先算点积，再取正弦。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e937b0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "q = torch.dot(x, y)\n",
    "z = torch.sin(q)\n",
    "print('z.requires_grad:', z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4d3da5",
   "metadata": {},
   "source": [
    "到这里你看到的依然只是数值计算，但 PyTorch 已经做了两件事：\n",
    "\n",
    "1. `z` 会自动变成需要梯度的结果（因为它依赖了需要梯度的 `x` 和 `y`）。\n",
    "2. `q` 和 `z` 的产生过程会被记录下来：`z` 由 `sin` 得到，`q` 由 `dot` 得到，而 `q` 又依赖 `x` 和 `y`。\n",
    "\n",
    "先别急着管计算图长什么样。我们先看一个更直观的现象：在你调用反向传播之前，梯度并不会凭空出现。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2eda64eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: None\n",
      "y.grad: None\n"
     ]
    }
   ],
   "source": [
    "print('x.grad:', x.grad)\n",
    "print('y.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3391b1a4",
   "metadata": {},
   "source": [
    "这里是 `None`，而不是 0。原因也很简单：梯度是一种反向回溯的产物，只有当你明确发起回溯（比如调用 `backward()`）时，PyTorch 才会沿着刚才记录的依赖关系，把梯度算出来并写回到叶子节点上。如果不调用，PyTorch 就不会去算梯度，自然也不会给你填上数值。\n",
    "\n",
    "接下来我们就做这件事：从 `z` 开始反向传播，看看 `.grad` 是如何出现的，以及它和我们手算的结果是否一致。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c07c86",
   "metadata": {},
   "source": [
    "## 2.1.2 backward 到底做了什么：从输出往回“查账”\n",
    "\n",
    "上一节我们只做了前向计算，但 PyTorch 已经把依赖关系悄悄记录好了。现在我们真正关心的是：当你调用 `backward()` 时，框架究竟做了什么？算出来的梯度又是否可信？\n",
    "\n",
    "还是沿用同一个例子：\n",
    "\n",
    "$$ q = x^\\top y, \\quad z = \\sin(q) $$\n",
    "\n",
    "如果我们手算梯度，我们就会得到：\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial q} \\cdot \\frac{\\partial q}{\\partial x} = \\cos(q) \\cdot y $$\n",
    "$$ \\frac{\\partial z}{\\partial y} = \\frac{\\partial z}{\\partial q} \\cdot \\frac{\\partial q}{\\partial y} = \\cos(q) \\cdot x $$\n",
    "\n",
    "好的，现在让 PyTorch 来算。我们直接从输出 `z` 发起回溯：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62cbabcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([3.1666, 3.7999, 4.4332, 5.0666])\n",
      "y.grad: tensor([0.6333, 1.2666, 1.9000, 2.5333])\n"
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print('x.grad:', x.grad)\n",
    "print('y.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892bac57",
   "metadata": {},
   "source": [
    "此时 `.grad` 不再是 `None`，梯度已经被写回到了 `x`、`y` 这两个叶子节点上。直觉上你可以这样理解 `backward()`：\n",
    "\n",
    "1. 以 `z` 为起点，默认认为 $\\frac{\\partial z}{\\partial z} = 1$；\n",
    "2. 然后沿着前向传播时记下来的依赖链往回走；\n",
    "3. 每走过一个算子节点，就用这个算子的局部求导规则把梯度继续往上游传递。\n",
    "\n",
    "我们可以把它和手算结果对齐。比如：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "757e46b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(x.grad, y * x.dot(y).cos())\n",
    "assert torch.allclose(y.grad, x * x.dot(y).cos())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71eb159",
   "metadata": {},
   "source": [
    "到这里，自动微分的核心逻辑其实已经很清楚了。深度学习框架并不需要推导一个巨大的全局导数公式，它只需要知道每一步怎么求导，然后把这些局部规则按计算图的结构串起来。\n",
    "\n",
    "如果再深入一点，其实 PyTorch 也把这条回溯链暴露了一部分给我们。比如：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccddf67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z.grad_fn: SinBackward0\n",
      "q.grad_fn: DotBackward0\n",
      "x.grad_fn: None\n",
      "y.grad_fn: None\n"
     ]
    }
   ],
   "source": [
    "print('z.grad_fn:', z.grad_fn.name())\n",
    "print('q.grad_fn:', q.grad_fn.name())\n",
    "print('x.grad_fn:', x.grad_fn)\n",
    "print('y.grad_fn:', y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a208347",
   "metadata": {},
   "source": [
    "我们通常会看到类似 `SinBackward0` 这样带有 `Backward` 的名字。它的含义可以粗略理解为：\n",
    "\n",
    "- `z` 不是凭空来的，它是某个算子（这里是 `sin`）产生的结果；\n",
    "- `grad_fn` 就是这个算子在反向传播时对应的梯度函数对象。\n",
    "\n",
    "在计算反向传播时，PyTorch 从根节点开始，依次调用每个节点的导数算子，计算出各个输入变量的梯度，直到到达输入节点为止。例如，当我们调用 `z.backward()` 时，PyTorch 会首先调用 `z` 节点的导数算子 `SinBackward0`，计算出 $\\frac{\\partial z}{\\partial q}$，然后将该值传递给 `q` 节点的导数算子 `DotBackward0`，计算出 $\\frac{\\partial q}{\\partial x}$ 和 $\\frac{\\partial q}{\\partial y}$，最终得到 $\\frac{\\partial z}{\\partial x}$ 和 $\\frac{\\partial z}{\\partial y}$。叶子节点（如 `x` 和 `y`）没有导数算子，因为它们是计算图的起点，不需要进一步计算梯度。\n",
    "\n",
    "更关键的是，`grad_fn.next_functions` 会指向它的上游依赖：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a72e740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_fn of z.child -> q: DotBackward0\n",
      "grad_fn of q.child -> x: struct torch::autograd::AccumulateGrad\n",
      "grad_fn of q.child -> y: struct torch::autograd::AccumulateGrad\n"
     ]
    }
   ],
   "source": [
    "node_q = z.grad_fn.next_functions[0][0]\n",
    "node_x = node_q.next_functions[0][0]\n",
    "node_y = node_q.next_functions[1][0]\n",
    "print('grad_fn of z.child -> q:', node_q.name())\n",
    "print('grad_fn of q.child -> x:', node_x.name())\n",
    "print('grad_fn of q.child -> y:', node_y.name())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49447bd3",
   "metadata": {},
   "source": [
    "它们描述的是，为了计算 `z` 的梯度，反向传播接下来应该去找谁、沿着哪些输入回溯。例如，在 `SinBackward0` 节点中，`next_functions` 会指向 `DotBackward0` 节点，因为 `SinBackward0` 的输入是 `q`，而 `q` 是通过 `DotBackward0` 计算得到的。同样地，在 `DotBackward0` 节点中，`next_functions` 会指向输入节点 `x` 和 `y`。`AccumulateGrad` 是一个特殊的节点类型，每个需要梯度的叶子节点前都会有一个对应的 `AccumulateGrad` 节点，负责把得到的梯度累加到叶子节点的 `.grad` 属性中。这也就是为什么 `x.grad`、`y.grad` 最终会在调用 `backward()` 后出现。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3e529c",
   "metadata": {},
   "source": [
    "## 2.1.3 为什么非标量不能直接 backward\n",
    "\n",
    "上面的例子里，`z` 是一个标量，所以我们可以理直气壮地写 `z.backward()`。相信很多人第一次换成输出是向量或者矩阵时，会立刻撞到 PyTorch 的一条看起来很不讲理的限制：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8937c3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: grad can be implicitly created only for scalar outputs\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1.0, 5.0, requires_grad=True)\n",
    "y = torch.arange(5.0, 9.0, requires_grad=True)\n",
    "Z = torch.outer(x, y)\n",
    "try:\n",
    "    Z.backward()  # This will raise an error because z is not a scalar\n",
    "except RuntimeError as err:\n",
    "    print('RuntimeError:', err)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0243b5b6",
   "metadata": {},
   "source": [
    "这不是 PyTorch 小气，而是反向传播的起点在非标量情况下不再唯一。\n",
    "\n",
    "对标量 `z`，我们通常关心的是 $\\frac{\\partial z}{\\partial x}$ 和 $\\frac{\\partial z}{\\partial y}$。反向传播从输出出发，第一步就是设定 $\\frac{\\partial z}{\\partial z} = 1$。这一步之所以合理，是因为标量输出的单位梯度没有歧义：我们就是要沿着 `z` 这个方向往回传。\n",
    "\n",
    "但是，如果输出是向量或者矩阵 `Z` 呢？我们到底想要什么？\n",
    "\n",
    "- 是想要 `Z` 的每一个元素对 `x` 和 `y` 的梯度吗？那会是一个更高阶的张量。\n",
    "- 还是想要某个标量函数，比如 `Z` 的和、均值、某个加权和，对 `x` 和 `y` 的梯度？\n",
    "\n",
    "也就是说，对非标量输出，反向传播必须先回答一句话：我们打算从哪个“方向”把梯度回传？\n",
    "\n",
    "在数学上，这个“方向”就是一个与输出同形状的张量 `v`，表示从上游传下来的梯度：\n",
    "\n",
    "$$ v = \\frac{\\partial L}{\\partial Z} $$\n",
    "\n",
    "然后 PyTorch 实际计算的是向量-雅可比积（VJP）：\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial x} = v^\\top \\left(\\frac{\\partial Z}{\\partial x}\\right) $$\n",
    "\n",
    "对于标量输出，`v` 自动为 1（等价于调用 `Z.backward()`，即把 $L$ 取为 $Z$）；对于非标量输出，`v` 需要我们自己提供。\n",
    "\n",
    "这里就有两种写法。\n",
    "\n",
    "一种写法是，我们显式传入 `gradient`，表示我们想要从哪个方向回传梯度：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ef9af67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([26., 26., 26., 26.])\n",
      "y.grad: tensor([10., 10., 10., 10.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1.0, 5.0, requires_grad=True)\n",
    "y = torch.arange(5.0, 9.0, requires_grad=True)\n",
    "Z = torch.outer(x, y)\n",
    "Z.backward(gradient=torch.ones_like(Z))\n",
    "print('x.grad:', x.grad)\n",
    "print('y.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b0c2d",
   "metadata": {},
   "source": [
    "这里 `torch.ones_like(Z)` 就是告诉 PyTorch，我想让 $L = \\sum_{i,j} Z_{i,j}$，因为\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial Z_{i,j}} = 1 $$\n",
    "\n",
    "所以传一个全 1 的梯度，就等价于“对所有元素求和后再 `backward`”。\n",
    "\n",
    "还有另外一种写法，就是先把 `Z` 变成一个标量，再对这个标量调用 `backward()`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974dd203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.grad: tensor([26., 26., 26., 26.])\n",
      "y.grad: tensor([10., 10., 10., 10.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.arange(1.0, 5.0, requires_grad=True)\n",
    "y = torch.arange(5.0, 9.0, requires_grad=True)\n",
    "Z = torch.outer(x, y)\n",
    "Z = torch.sum(Z)\n",
    "Z.backward()\n",
    "print('x.grad:', x.grad)\n",
    "print('y.grad:', y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00fe68",
   "metadata": {},
   "source": [
    "这两种写法在很多情况下是等价的。要么我们显式告诉 PyTorch 从哪个方向回传梯度，要么我们先把输出变成一个标量（比如求和），让它自己默认从这个标量的方向回传梯度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a50d21",
   "metadata": {},
   "source": [
    "## 2.1.4 高阶导数：让求导过程也变成计算的一部分\n",
    "\n",
    "到目前为止，我们做的都是一阶梯度：给定一个标量输出（或者可以转换成标量输出）$L$，求 $\\nabla_x L$，$\\nabla_y L$。但有时候我们会需要更高阶的信息，比如二阶导数（Hessian 的某些方向）、曲率、或者用在一些正则项里。\n",
    "\n",
    "那么这件事的关键点在于：如果你想对“梯度”再求导，那么“求梯度这件事”本身也必须是可微的。这就是 `create_graph=True` 的含义。在计算一阶导数时，不仅算出数值，还要把“算出这个导数的过程”记录成新的计算图。\n",
    "\n",
    "可能这时候很多人就会有疑惑，为什么不用 `backward()` 呢？因为 `backward()` 的设计目标是训练模型：我们把梯度累积进叶子张量的 `.grad` 属性中，并且默认释放图来节省内存。但是，在做高阶导时，我们更希望：\n",
    "\n",
    "- 梯度作为一个张量返回（方便继续算）\n",
    "- 必要时保留 / 构建计算图（方便再求导）\n",
    "\n",
    "因此更常用的是 `torch.autograd.grad`。\n",
    "\n",
    "我们还是用上面的例子：$z = \\sin(x \\cdot y)$。我们先求一阶导数 $\\frac{dz}{dx}$ 和 $\\frac{dz}{dy}$，然后再对这个结果求导，看看二阶导数 $\\frac{d^2 z}{dx^2}$ 和 $\\frac{d^2 z}{dy^2}$ 是什么样的。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c793f16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(-0.5820, grad_fn=<MulBackward0>)\n",
      "dz/dy: tensor(-0.2910, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(4.0, requires_grad=True)\n",
    "z = torch.sin(x * y)\n",
    "\n",
    "dzdx, dzdy = torch.autograd.grad(z, (x, y), create_graph=True)\n",
    "print('dz/dx:', dzdx)\n",
    "print('dz/dy:', dzdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda58929",
   "metadata": {},
   "source": [
    "这里最重要的一行是 `create_graph=True`。如果没有它，`dz/dx` 和 `dz/dy` 会被当成纯数值结果，不再保留它是怎么得到的。那我们就没法再对它求导。`dz/dx` 和 `dz/dy` 的输出都包含了一个 `grad_fn`，说明他们允许自身被求导。\n",
    "\n",
    "在计算高阶导数时，我们有时候希望在同一个计算图中前后对不同变量分别求导。但是，PyTorch 在调用一次 `backward()` 后默认会释放计算图来节省内存，这就导致我们无法在同一个图里连续求导。如果我们确实需要在同一次前向结果上做多次回溯，可以通过设置 `retain_graph=True` 来保留图：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5512cdbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/dx: tensor(-0.5820, grad_fn=<MulBackward0>)\n",
      "dz/dy: tensor(-0.2910, grad_fn=<MulBackward0>)\n",
      "d2z/dx2: tensor(-15.8297)\n",
      "d2z/dy2: tensor(-3.9574)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = torch.tensor(4.0, requires_grad=True)\n",
    "z = torch.sin(x * y)\n",
    "\n",
    "dzdx, dzdy = torch.autograd.grad(z, (x, y), create_graph=True)\n",
    "print('dz/dx:', dzdx)\n",
    "print('dz/dy:', dzdy)\n",
    "\n",
    "(d2zdx2,) = torch.autograd.grad(dzdx, x, retain_graph=True)\n",
    "(d2zdy2,) = torch.autograd.grad(dzdy, y)\n",
    "print('d2z/dx2:', d2zdx2)\n",
    "print('d2z/dy2:', d2zdy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4272db",
   "metadata": {},
   "source": [
    "不过更常见的做法是，重新执行一次前向传播来得到一张新的计算图。`retain_graph=True` 通常是当我们确实要在同一个计算图上做多次梯度计算时才用，比如高阶导数实验或者某些正则项的计算。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee745fc6",
   "metadata": {},
   "source": [
    "## 2.1.5 VJP 和 JVP：反向模式与正向模式到底在算什么\n",
    "\n",
    "到目前为止我们一直在说“求梯度”。但严格来说，深度学习里绝大多数函数并不是从标量到标量，而是：\n",
    "\n",
    "$$ f: \\mathbb{R}^n \\to \\mathbb{R}^m $$\n",
    "\n",
    "它的导数是一个雅可比矩阵（Jacobian）：\n",
    "\n",
    "$$ J = \\frac{\\partial f}{\\partial x} \\in \\mathbb{R}^{m \\times n} $$\n",
    "\n",
    "真正的问题是，当 $m,n$ 都很大时，我们几乎从来不会显式构造 $J$。我们真正想要的，框架实际计算的是 Jacobian 的乘积，要么乘在左边，要么乘在右边。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4c3a3",
   "metadata": {},
   "source": [
    "### 2.1.5.1 VJP：向量-雅可比积（反向模式）\n",
    "\n",
    "给定“上游梯度”向量 $v \\in \\mathbb{R}^m$（可以理解为 $\\frac{\\partial L}{\\partial f}$），反向模式计算的是：\n",
    "\n",
    "$$ v^\\top J \\in \\mathbb{R}^n $$\n",
    "\n",
    "这就是 **VJP（vector-Jacobian product）**。\n",
    "\n",
    "把它翻译成训练时的语言就更熟悉了：\n",
    "\n",
    "- 我们有一个标量 `loss`：$L = \\mathcal{L}(f(x))$\n",
    "- 一个上游梯度：$v = \\frac{\\partial L}{\\partial f}$\n",
    "- 进行反向传播：$\\frac{\\partial L}{\\partial x} = v^\\top \\frac{\\partial f}{\\partial x}$\n",
    "\n",
    "所以，平时我们调用 `backward()`，实际上就是在计算一个特殊的 VJP。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa45c237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func(x,y): tensor(0.7739)\n",
      "VJP output: (tensor([3.1666, 3.7999, 4.4332, 5.0666]), tensor([0.6333, 1.2666, 1.9000, 2.5333]))\n"
     ]
    }
   ],
   "source": [
    "def vjp_func(x: torch.Tensor, y: torch.Tensor):\n",
    "    return torch.sin(torch.dot(x, y))\n",
    "\n",
    "\n",
    "x = torch.arange(1.0, 5.0)\n",
    "y = torch.arange(5.0, 9.0)\n",
    "out = AF.vjp(vjp_func, (x, y))\n",
    "print('func(x,y):', out[0])\n",
    "print('VJP output:', out[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcfb5b8",
   "metadata": {},
   "source": [
    "### 2.1.5.2 JVP：雅可比-向量积（正向模式）\n",
    "\n",
    "正向模式则相反：给定一个输入方向 $u \\in \\mathbb{R}^n$，计算：\n",
    "\n",
    "$$ Ju \\in \\mathbb{R}^m $$\n",
    "\n",
    "这就是 **JVP（Jacobian-vector product）**。从直觉上，它回答的问题是：如果我们在输入空间里沿某个方向 $u$ 做一个微小的扰动，输出会沿着哪个方向变化？这在做敏感性分析、隐式层、某些二阶方法、以及一些物理/科学计算中非常常见。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1a51552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func(x,y): tensor(0.7739)\n",
      "JVP output: tensor(2.9133)\n"
     ]
    }
   ],
   "source": [
    "def jvp_func(a: torch.Tensor, b: torch.Tensor):\n",
    "    return torch.sin(torch.dot(a, b))\n",
    "\n",
    "\n",
    "x = torch.arange(1.0, 5.0)\n",
    "y = torch.arange(5.0, 9.0)\n",
    "v_x = torch.full_like(x, 0.1)\n",
    "v_y = torch.full_like(y, 0.2)\n",
    "out = AF.jvp(jvp_func, (x, y), (v_x, v_y))\n",
    "print('func(x,y):', out[0])\n",
    "print('JVP output:', out[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ab0c2d",
   "metadata": {},
   "source": [
    "### 2.1.5.3 为什么深度学习里更常见的是 VJP\n",
    "\n",
    "这个问题不是“谁更高级”，而是”规模匹配”。\n",
    "\n",
    "- 在深度学习训练中，通常 $n$ 是参数维度（百万/亿级），$m$ 是输出维度（通常是一个标量）\n",
    "- 我们真正想要的是 $\\nabla L \\in \\mathbb{R}^n$\n",
    "\n",
    "VJP 的复杂度大致和“一次反向传播”同量级，适合 $n$ 很大但输出是标量/低维的场景。JVP 更适合输入维度相对小，但我们关心输出方向变化的场景。所以，我们会看到一个很经典的判断：如果输出是标量或低维向量，而且输入维度很大，那么反向模式（VJP）更合适；如果输入维度相对较小，输出维度很大，那么正向模式（JVP）可能更合适。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8126a6",
   "metadata": {},
   "source": [
    "## 2.1.6 反向传播中的常见错误\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "578f62d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(1.0, 5.0, requires_grad=True)\n",
    "y = torch.arange(5.0, 9.0, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2d5207",
   "metadata": {},
   "source": [
    "1. 重复调用 `backward()`：在同一个计算图上多次调用 `backward()` 会导致错误。PyTorch 在第一次反向传播结束后，会把这张图里“只为反向传播服务的中间变量”释放掉，以节省显存。所以当我们第二次再沿着同一张图回溯，就会发现“路标”已经被清理了。如果需要多次计算梯度，可以在第一次调用时设置 `retain_graph=True`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfa70386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.\n"
     ]
    }
   ],
   "source": [
    "z = torch.sin(torch.dot(x, y))\n",
    "z.backward()\n",
    "try:\n",
    "    z.backward()  # This will raise an error because gradients are already computed\n",
    "except RuntimeError as err:\n",
    "    print('RuntimeError:', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0d93e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.sin(torch.dot(x, y))\n",
    "z.backward(retain_graph=True)\n",
    "z.backward()  # This works because we retained the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceda4bd",
   "metadata": {},
   "source": [
    "2. 尝试访问中间节点的梯度：只有叶子节点（即最初创建的变量）会存储梯度信息。中间节点的梯度不会被存储，因为如果每个中间变量都存梯度，显存会直接爆炸，而且训练真正需要的是参数梯度，而不是所有中间量的梯度。因此尝试访问它们的 `.grad` 属性会返回 `None`，并引发 `UserWarning`。如果需要保留中间节点的梯度，可以在创建这些节点时设置 `q.retain_grad()`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5819a95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.grad: None\n",
      "UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:497.)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "q = torch.dot(x, y)\n",
    "z = torch.sin(q)\n",
    "z.backward()\n",
    "\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    print('q.grad:', q.grad)\n",
    "    if len(w) > 0:\n",
    "        for warn in w:\n",
    "            print('UserWarning:', warn.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8849d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q.grad after retain_grad: tensor(0.6333)\n"
     ]
    }
   ],
   "source": [
    "q = torch.dot(x, y)\n",
    "q.retain_grad()\n",
    "z = torch.sin(q)\n",
    "z.backward()\n",
    "print('q.grad after retain_grad:', q.grad)  # Now q.grad is available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc903e3",
   "metadata": {},
   "source": [
    "3. 使用原地操作：PyTorch 里像 `x.add_(1)`、`x.relu_()` 这种带下划线的操作，表示原地修改张量。不创建新张量，而是直接改 `x` 自己的内存。这在直觉上很省事，但在反向传播往往需要用到前向传播时的某些中间值。如果这些值在前向之后被我们就地改掉，那反向传播就可能失去计算梯度所需的信息。因此，在反向传播过程中，尽量避免使用原地操作，或者确保它们不会修改反向传播需要的中间变量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42c36582",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n"
     ]
    }
   ],
   "source": [
    "z = torch.dot(x, y)\n",
    "try:\n",
    "    x.relu_()\n",
    "except RuntimeError as err:\n",
    "    print('RuntimeError:', err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9571ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.dot(x, y)\n",
    "x = torch.relu(x)\n",
    "z.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
